{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b8632f9-b953-47c5-8045-542c52621be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: requests==2.28.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 2)) (2.28.0)\n",
      "Requirement already satisfied: BeautifulSoup4==4.11.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 3)) (4.11.1)\n",
      "Requirement already satisfied: Wikipedia-Api==0.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: trafilatura==0.9.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: lxml_html_clean==0.4.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: openai==1.55.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 8)) (1.55.3)\n",
      "Requirement already satisfied: httpx==0.27.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 9)) (0.27.2)\n",
      "Requirement already satisfied: asyncio==3.4.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 10)) (3.4.3)\n",
      "Requirement already satisfied: scipy==1.15.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 11)) (1.15.2)\n",
      "Collecting newspaper4k (from -r requirements.txt (line 12))\n",
      "  Downloading newspaper4k-0.9.3.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from BeautifulSoup4==4.11.1->-r requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: courlan>=0.4.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=0.9.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (1.2.3)\n",
      "Requirement already satisfied: justext>=2.2.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: lxml>=4.6.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (5.3.1)\n",
      "Requirement already satisfied: readability-lxml>=0.8.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: chardet>=4.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 5)) (5.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from openai==1.55.3->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from httpx==0.27.2->-r requirements.txt (line 9)) (1.0.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from scipy==1.15.2->-r requirements.txt (line 11)) (2.2.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from httpcore==1.*->httpx==0.27.2->-r requirements.txt (line 9)) (0.14.0)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: feedparser>=6.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (6.0.11)\n",
      "Requirement already satisfied: nltk>=3.6.6 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (3.9.1)\n",
      "Collecting pandas>=2.1.0 (from newspaper4k->-r requirements.txt (line 12))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper4k->-r requirements.txt (line 12)) (5.1.3)\n",
      "Requirement already satisfied: babel>=2.16.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from courlan>=0.4.1->trafilatura==0.9.0->-r requirements.txt (line 5)) (2.16.0)\n",
      "Requirement already satisfied: tld>=0.13 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from courlan>=0.4.1->trafilatura==0.9.0->-r requirements.txt (line 5)) (0.13)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from feedparser>=6.0.0->newspaper4k->-r requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: dateparser>=1.1.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.6.6->newspaper4k->-r requirements.txt (line 12)) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.6.6->newspaper4k->-r requirements.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.6.6->newspaper4k->-r requirements.txt (line 12)) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from pandas>=2.1.0->newspaper4k->-r requirements.txt (line 12)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from pandas>=2.1.0->newspaper4k->-r requirements.txt (line 12)) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.55.3->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.55.3->-r requirements.txt (line 8)) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from python-dateutil>=2.6.1->newspaper4k->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: cssselect in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from readability-lxml>=0.8.1->trafilatura==0.9.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tldextract>=2.0.1->newspaper4k->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tldextract>=2.0.1->newspaper4k->-r requirements.txt (line 12)) (3.17.0)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from dateparser>=1.1.1->htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 5)) (5.3.1)\n",
      "Downloading newspaper4k-0.9.3.1-py3-none-any.whl (296 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.5 MB 5.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.5 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.5 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas, newspaper4k\n",
      "Successfully installed newspaper4k-0.9.3.1 pandas-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10fe472-7e94-41f7-843b-5a33f1eb770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a4b5ec-3805-4a98-9942-d5b12d1bb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6664a03f-3846-48ae-8bcc-6377651f84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216bea97-a5c3-464b-8015-5d87020be620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dagri\\anaconda3\\envs\\WikiBench\\Lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "C:\\Users\\dagri\\anaconda3\\envs\\WikiBench\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from wiki_extract import Extracter\n",
    "from wiki_gen import WikiGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b4a312-18dc-4047-acd8-da3ea801094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(name):\n",
    "    print('Article name: ', name)\n",
    "    page = Extracter(name)\n",
    "    page.get_references()\n",
    "    page.get_outline()\n",
    "    page.get_reference_positions()\n",
    "    page.newspaper_ref()\n",
    "    page.get_filtered_outline()\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21955e3-1b7f-4abc-ac9f-e5ca0ccabaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = [\n",
    "    'Python',\n",
    "    'Летние Олимпийские игры 2024',\n",
    "    'Квантовый компьютер',\n",
    "    'Присоединение Крыма к Российской Федерации',\n",
    "    'Сколково (инновационный центр)',\n",
    "    'Tomb Raider (игра, 2013)',\n",
    "    'Чёрная дыра',\n",
    "    'Экономика США',\n",
    "    'Искусственный интеллект',\n",
    "    'COVID-19',\n",
    "    'Броненосный крейсер',\n",
    "    'Применение искусственного интеллекта',\n",
    "    'РИА Новости'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92438061-70a2-4ca0-8ca3-460d25428830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_texts(article_name, texts, number):\n",
    "\n",
    "    directory = os.path.join(\"Articles\", \"Sources\", article_name)\n",
    "\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    '''\n",
    "    for i, text in enumerate(texts, start=number):\n",
    "        file_path = os.path.join(directory, f\"source_{i}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a0577b-98af-4a5c-92f9-35f697e0bd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article name:  COVID-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating outline: 100%|██████████| 191/191 [00:00<00:00, 38450.30it/s]\n",
      "Getting link numbers: 100%|██████████| 257/257 [00:02<00:00, 118.88it/s]\n",
      "Calculating reference positions: 100%|██████████| 57/57 [00:00<00:00, 57332.21it/s]\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Retrieving sources: 100%|██████████| 257/257 [51:18<00:00, 11.98s/it]\n",
      "Getting link numbers: 100%|██████████| 257/257 [00:03<00:00, 69.54it/s]\n",
      "Calculating reference positions: 100%|██████████| 57/57 [00:00<00:00, 56963.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article name:  Броненосный крейсер\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating outline: 100%|██████████| 282/282 [00:00<00:00, 40277.66it/s]\n",
      "Getting link numbers: 100%|██████████| 180/180 [00:01<00:00, 110.23it/s]\n",
      "Calculating reference positions: 100%|██████████| 31/31 [00:00<00:00, 23949.79it/s]\n",
      "Retrieving sources: 100%|██████████| 180/180 [01:49<00:00,  1.64it/s]\n",
      "Getting link numbers: 100%|██████████| 180/180 [00:01<00:00, 120.29it/s]\n",
      "Calculating reference positions: 100%|██████████| 31/31 [00:00<00:00, 30994.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article name:  Применение искусственного интеллекта\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating outline: 100%|██████████| 151/151 [00:00<00:00, 42926.66it/s]\n",
      "Getting link numbers: 100%|██████████| 224/224 [00:01<00:00, 133.01it/s]\n",
      "Calculating reference positions: 100%|██████████| 48/48 [00:00<00:00, 90362.03it/s]\n",
      "Retrieving sources: 100%|██████████| 224/224 [15:20<00:00,  4.11s/it] \n",
      "Getting link numbers: 100%|██████████| 224/224 [00:01<00:00, 211.95it/s]\n",
      "Calculating reference positions: 100%|██████████| 48/48 [00:00<00:00, 48049.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article name:  РИА Новости\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating outline: 100%|██████████| 102/102 [00:00<00:00, 50031.46it/s]\n",
      "Getting link numbers: 100%|██████████| 83/83 [00:00<00:00, 596.51it/s]\n",
      "Calculating reference positions: 100%|██████████| 31/31 [00:00<00:00, 31002.25it/s]\n",
      "Retrieving sources: 100%|██████████| 83/83 [20:11<00:00, 14.60s/it]\n",
      "Getting link numbers: 100%|██████████| 83/83 [00:00<00:00, 598.82it/s]\n",
      "Calculating reference positions: 100%|██████████| 31/31 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = {}\n",
    "\n",
    "for name in article_names:\n",
    "    page = get_article(name)\n",
    "    articles[name] = page.ref_texts.keys()\n",
    "    num = 1\n",
    "    for _, texts in page.ref_texts.items():\n",
    "        save_texts(page.name, texts, num)\n",
    "        num += len(texts)\n",
    "\n",
    "with open('saved_ref.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c830643c-8a34-410c-bd43-5997626e4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "oclient = OpenAI(api_key=KEY, base_url=URL)\n",
    "\n",
    "wiki_writer = WikiGen(oclient)\n",
    "\n",
    "for name in article_names:\n",
    "\n",
    "    json_str = wiki_writer.get_subqueries(name)\n",
    "\n",
    "    if json_str:\n",
    "        data = json.loads(json_str)\n",
    "        directory = os.path.join(\"Generation\", \"Subqueries\", name)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        filename = name.replace(\" \", \"_\") + '.json'\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85fcd875-e500-442d-b298-a1d04536415e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8.099553600038334e-08), (1, 0.9999917033166356), (2, 1.3164813461585823e-08), (3, 1.2751017020029565e-08), (4, 3.096424074922055e-07), (5, 4.9639593324712905e-08), (6, 4.5885568344949945e-08), (7, 9.091663710769637e-08), (8, 3.884208066295969e-08), (9, 4.281467835820507e-08), (10, 2.609354399218944e-08), (11, 2.798294017059e-06), (12, 2.3039322782913985e-08), (13, 4.183454227479899e-08), (14, 9.596253980337366e-08), (15, 3.1558730184677586e-08), (16, 3.248581048609367e-08), (17, 1.0367625347118548e-07), (18, 2.0683136703336658e-07), (19, 7.234636416608708e-07), (20, 7.344533614617177e-08), (21, 5.73288518879167e-08), (22, 0.9999958264024809), (23, 0.9998447536121327), (24, 0.9999789668010084), (25, 1.3164813461585823e-08)]\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "# ----- Основной код -----\n",
    "KEY = '874c364705747e7ab314ceba89c2029c9a72ab2154664c470eb4ce18c2f0acb0'\n",
    "URL = \"http://demo.labinform.ru:30101/v1\"\n",
    "client = AsyncOpenAI(api_key=KEY, base_url=URL)\n",
    "wiki_writer = WikiGen(client)\n",
    "\n",
    "topic = 'COVID-19'\n",
    "\n",
    "texts = []\n",
    "file_names = []\n",
    "directory = 'Articles/Sources/Экономика США'\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "        file_names.append(file)\n",
    "\n",
    "lst = await wiki_writer.filter_sources(topic, \"\", texts)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e8eb33-6f3c-49a3-b779-5e4a52a0eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c553eec4-12da-421f-9882-a52fd5e170f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "394b9d34-95b2-4bdd-bc32-9f6e1edf5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"article_title\": \"COVID-19\",\n",
      "  \"subqueries\": [\n",
      "    \"Каковы основные пути передачи вируса SARS-CoV-2 и какие меры предотвращения наиболее эффективны?\",\n",
      "    \"Какие симптомы и клинические проявления характерны для COVID-19, и как они различаются у разных групп населения?\",\n",
      "    \"Какие методы диагностики COVID-19 существуют, и какие из них наиболее точны и доступны?\",\n",
      "    \"Какие вакцины против COVID-19 были разработаны, и каковы их эффективность и побочные эффекты?\",\n",
      "    \"Какие экономические последствия вызвал пандемический кризис, и какие меры поддержки были приняты правительствами?\",\n",
      "    \"Как пандемия COVID-19 повлияла на образование, работу и социальные взаимодействия?\",\n",
      "    \"Какие научные исследования проводятся для изучения долгосрочных последствий COVID-19 для здоровья?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(await wiki_writer.get_subqueries('COVID-19'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e04a6-ced3-4c86-a89a-c867702c4438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
