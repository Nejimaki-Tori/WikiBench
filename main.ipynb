{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8632f9-b953-47c5-8045-542c52621be8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia==1.4.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: requests==2.28.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 2)) (2.28.0)\n",
      "Requirement already satisfied: BeautifulSoup4==4.11.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 3)) (4.11.1)\n",
      "Requirement already satisfied: Wikipedia-Api==0.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: newspaper3k==0.2.8 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: trafilatura==0.9.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: lxml_html_clean==0.4.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 7)) (0.4.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from requests==2.28.0->-r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from BeautifulSoup4==4.11.1->-r requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (5.3.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (3.9.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from newspaper3k==0.2.8->-r requirements.txt (line 5)) (0.3)\n",
      "Requirement already satisfied: courlan>=0.4.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=0.9.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 6)) (1.2.3)\n",
      "Requirement already satisfied: justext>=2.2.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: readability-lxml>=0.8.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 6)) (0.8.1)\n",
      "Requirement already satisfied: chardet>=4.0.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from trafilatura==0.9.0->-r requirements.txt (line 6)) (5.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: babel>=2.16.0 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from courlan>=0.4.1->trafilatura==0.9.0->-r requirements.txt (line 6)) (2.17.0)\n",
      "Requirement already satisfied: tld>=0.13 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from courlan>=0.4.1->trafilatura==0.9.0->-r requirements.txt (line 6)) (0.13)\n",
      "Requirement already satisfied: six in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k==0.2.8->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: dateparser>=1.1.1 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.2.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.2.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from nltk>=3.2.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k==0.2.8->-r requirements.txt (line 5)) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from dateparser>=1.1.1->htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 6)) (2025.1)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from dateparser>=1.1.1->htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 6)) (5.3)\n",
      "Requirement already satisfied: tzdata in c:\\users\\dagri\\anaconda3\\envs\\wikibench\\lib\\site-packages (from tzlocal>=0.2->dateparser>=1.1.1->htmldate>=0.9.0->trafilatura==0.9.0->-r requirements.txt (line 6)) (2025.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f10fe472-7e94-41f7-843b-5a33f1eb770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a4b5ec-3805-4a98-9942-d5b12d1bb298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6664a03f-3846-48ae-8bcc-6377651f84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216bea97-a5c3-464b-8015-5d87020be620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_extract import Extracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d23e25b4-c952-4ee4-b609-75b4d0b70cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = Extracter('DeepSeek (языковая модель)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d00b7e-6356-406c-8359-6161643490c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page.get_references()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b524c72-5b19-48f9-89a1-4f34ba97b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating outline: 100%|██████████| 36/36 [00:00<00:00, 36011.20it/s]\n"
     ]
    }
   ],
   "source": [
    "page.get_outline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e30700c-4571-403c-8cb6-a608d24313bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting link numbers: 100%|██████████| 31/31 [00:00<00:00, 756.06it/s]\n",
      "Calculating reference positions: 100%|██████████| 14/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cite_note-:7-1': (('h3', 'DeepSeek-V3'), 1),\n",
       " 'cite_note-:2-2': (('h3', 'DeepSeek-V3'), 2),\n",
       " 'cite_note-:8-3': (('h3', 'DeepSeek-V3'), 1),\n",
       " 'cite_note-:1-4': (('h3', 'DeepSeek-R1'), 1),\n",
       " 'cite_note-:3-5': (('h1', 'DeepSeek (языковая модель)'), 3),\n",
       " 'cite_note-6': (('h1', 'DeepSeek (языковая модель)'), 3),\n",
       " 'cite_note-:5-7': (('h3', 'Санкции США как катализатор инноваций'), 1),\n",
       " 'cite_note-8': (('h1', 'DeepSeek (языковая модель)'), 4),\n",
       " 'cite_note-9': (('h1', 'DeepSeek (языковая модель)'), 4),\n",
       " 'cite_note-12': (('h3', 'Условия распространения продуктов DeepSeek'), 1),\n",
       " 'cite_note-:4-13': (('h3', 'DeepSeek Coder и DeepSeek LLM'), 1),\n",
       " 'cite_note-:0-16': (('h3', 'DeepSeek-V2'), 3),\n",
       " 'cite_note-17': (('h3', 'DeepSeek-V2'), 3),\n",
       " 'cite_note-18': (('h3', 'DeepSeek-V2'), 3),\n",
       " 'cite_note-20': (('h3', 'DeepSeek-V2'), 4),\n",
       " 'cite_note-21': (('h3', 'DeepSeek-V2'), 4),\n",
       " 'cite_note-22': (('h3', 'DeepSeek-V3'), 2),\n",
       " 'cite_note-23': (('h3', 'DeepSeek-V3'), 3),\n",
       " 'cite_note-25': (('h3', 'DeepSeek-R1'), 1),\n",
       " 'cite_note-26': (('h3', 'DeepSeek-R1'), 1),\n",
       " 'cite_note-27': (('h3', 'DeepSeek-R1'), 1),\n",
       " 'cite_note-28': (('h3', 'DeepSeek-R1'), 1),\n",
       " 'cite_note-:6-29': (('h4', 'Падение рынков в январе 2025\\xa0года'), 2),\n",
       " 'cite_note-31': (('h2', 'Обвинение в краже технологий'), 1),\n",
       " 'cite_note-32': (('h2', 'Обвинение в краже технологий'), 1),\n",
       " 'cite_note-33': (('h3', 'Санкции США как катализатор инноваций'), 1),\n",
       " 'cite_note-34': (('h3', 'Санкции США как катализатор инноваций'), 1),\n",
       " 'cite_note-36': (('h3', 'Санкции США как катализатор инноваций'), 1),\n",
       " 'cite_note-37': (('h3', 'Санкции США как катализатор инноваций'), 1)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.get_reference_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce42872d-e484-43ab-8c90-583f8c7513c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2be4855-5aad-4b73-a554-537ed16731f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek (языковая модель)\n",
      "2 ноября 2023 (DeepSeek Coder)\n",
      "\n",
      "DeepSeek — семейство языковых моделей с открытым исходным кодом, разрабатываемое одноимённой китайской лабораторией искусственного интеллекта. Новейшие версии большой языковой модели с чат-ботом — DeepSeek-V3 и «думающая» DeepSeek-R1 для решения логических задач — демонстрируют равные или лучшие показатели в обработке естественных языков, математических задачах и программировании по сравнению с разработками конкурентов ( GPT-4о или Open AI-o1 соответственно, а также аналогичные модели от других западных компаний).\n",
      "\n",
      "При этом, по утверждению DeepSeek, их модели требуют на порядок меньше ресурсов для обучения и поддержания работы, а также не ограничивают доступ для пользователей из России и любых других стран мира. Кроме того, стоимость использования моделей в среднем на 96 % ниже, чем у западных конкурентов.\n",
      "\n",
      "Выход на рынок модели DeepSeek-R1 20 января 2025 года вызвал масштабный обвал рыночных котировок технологических компаний США (как работающих в сфере искусственного интеллекта, так и производителей вычислительного оборудования), а также спровоцировал дискуссию об обоснованности многомиллиардных вложений в западную ИИ-инфраструктуру, завышенных требованиях к оборудованию и его стоимости.\n",
      "История развития\n",
      "DeepSeek был выделен в самостоятельный стартап из ИИ-направления китайского хедж-фонда High-Flyer, основанного в 2015 году китайским миллиардером и энтузиастом в сфере ИИ Ляном Вэньфаном. В июле 2024 года он заявлял об ограниченности современных генеративных моделей, а задачей новой компании обозначал достижение общего искусственного интеллекта (AGI). В ноябре 2024 года глава компании ещё раз подтвердил свою приверженность идее создания продуктов с открытым исходным кодом и отсутствии планов по какому-либо дальнейшему расширению платных услуг DeepSeek.\n",
      "Условия распространения продуктов DeepSeek\n",
      "В отличие от ведущих западных компаний в области искусственного интеллекта ( OpenAI, Meta или Anthropic ) все языковые модели DeepSeek с самого начала распространялись под свободной лицензией MIT. Комментируя успех DeepSeek V3 и DeepSeek R-1 в январе 2025 года главный специалист Meta по искусственному интеллекту Ян Лекун заявил, что «в сфере ИИ модели с открытым исходным кодом доказали своё превосходство над проприетарными решениями».\n",
      "Релизы\n",
      "С ноября 2023 года DeepSeek представила 3 генерации больших языковых моделей.\n",
      "DeepSeek Coder и DeepSeek LLM\n",
      "Первая нейросеть от DeepSeek Coder была представлена 2 ноября 2023 года. 29 ноября вышла первая универсальная большая языковая модель DeepSeek LLM с 67 млрд параметров, которая на тот момент превосходила возможности LLama 2 и приближалась к GPT-4, однако, по некоторым данным, имела проблемы с масштабируемостью и вычислительной эффективностью. Тогда же был впервые представлен интеллектуальный чат-бот, работающий на основе DeepSeek LLM.\n",
      "\n",
      "Всего было разработано 8 вариантов первой модели: четыре стандартных предобученных (Base) и четыре прошедших тонкую настройку набором инструкций (Instruct). Все они опирались на трансформерную архитектуру, использующую механизм « внимания », схожую с той что используют модели Llama.\n",
      "DeepSeek-V2\n",
      "В мае 2024 года DeepSeek выпустила вторую версию языковой модели в четырёх вариантах: стандартная (V2), уменьшенная (V2-Lite), а также стандартный чат-бот (V2-Chat) и его уменьшенная версия (V2-Chat-Lite).\n",
      "\n",
      "Количество параметров в старших моделях возросло до 236 млрд; предобучение проводилось на 8,1 трлн токенов, а максимальная длина контекста возросла до 128 тыс. токенов.\n",
      "\n",
      "Модель претерпела значительные архитектурные изменения по сравнению с первой версией: в ней применён инновационный метод машинного обучения на основе низкоранговой аппроксимации Multi-head Learning Attention ( MLA, с англ. — «Обучение со множественным вниманием»), позволивший многократно уменьшить стоимость и время обучения модели. Кроме того компания разработала и внедрила в модель доработанный принцип Mixture of Experts ( MoE, с англ. — « смесь экспертов »): при таком подходе модель состоит из большого числа подсетей, каждая из которых отвечает за свою специализированную область знаний и подключается к поиску ответа только по необходимости.\n",
      "\n",
      "Комбинация MLA и MoE позволила установить стоимость обработки миллиона токенов в 2 юаня, в то время как у ChatGPT она равнялась 2,5 долларам за аналогичный объём информации. При этом DeepSeek-V2 демонстрировала конкурентоспособное качество работы: лаборатории искусственного интеллекта при университете Ватерлоо поместила на 7 место в рейтинге лучших больших языковых моделей.\n",
      "DeepSeek-V3\n",
      "В декабре 2024 года в открытый доступ вышла третья генерация модели в двух вариантах: стандартная (V3-Base) и чат-бота (V3), содержащая 671 млрд параметров и проходившая обучение на 14,8 трлн токенов. Нейросеть, построенная на апробированной в DeepSeek-V2 архитектуре, способна анализировать и пересказывать тексты, выделяя главное, делать переводы, а также решать математические задачи и писать программы на одном уровне с наиболее продвинутыми моделями от OpenAI, Meta или Anthropic: так DeepSeek-V3 превосходит в тестах Llama 3.1 Qwen 2.5 и соответствует уровню GPT-4о и Claude 3.5 Sonnet.\n",
      "\n",
      "Представленное DeepSeek бесплатное приложение — чат-бот «DeepSeek — AI Assistant» — к концу января 2025 стало самым скачиваемым в мире, а также обошло ChatGPT в рейтинге самых высокооценённых бесплатных приложений в США. Кроме того DeepSeek не поддерживает политику санкций и никак не ограничивает доступ к своей модели, предоставляя равные возможности для пользователей из всех стран мира — в том числе России.\n",
      "\n",
      "При этом, по утверждению разработчиков модели, время её обучения составило всего лишь 55 дней на массиве из около 2000 урезанных для соблюдения требований экспортного контроля видеокарт Nvidia; таким образом стоимость обучения составила всего 5,5 млн долларов, как минимум на порядок раз ниже, чем Llama или GPT-4o.\n",
      "Особенности модели\n",
      "Таких показателей эффективности удалось добиться благодаря особенностям архитектуры DeepSeek:\n",
      "DeepSeek-R1\n",
      "20 января 2025 года DeepSeek представил свою думающую модель DeepSeek R1 для решения логических и математических задач с производительностью в математических тестах AIME и MATH на уровне флагманского решения Open AI-o1. Отличительная особенность этой модели заключается в пошаговой генерации ответов, повторяющей процесс мышления у человека. При её разработке компания использовала новый, более эффективный подход к моделированию вознаграждения при обучении с подкреплением.\n",
      "Падение рынков в январе 2025 года\n",
      "Появление модели, которая требует многократно меньших затрат для обучения и использования, привела к обрушению котировок технологических компаний 28 января 2025 года. Так главный мировой поставщик оборудования для обучения нейросетей Nvidia потерял свыше 600 млрд долларов или почти 18 % капитализации, что стало крупнейшим обвалом в истории фондового рынка.\n",
      "\n",
      "Существенные потери понесли практически все связанные с сектором ИИ компании — Nebius Аркадия Воложа (-4 %) Broadcom (-17,3 %), AMD (-8 %), Palantir (-7 %), Microsoft (-3 %), а также основные поставщики электроэнергии для дата-центров Constellation Energy (-21 %) и Vistra (-29 %). Индекс NASDAQ просел на 3,5 %, а S&P на 1,8 %. Суммарно же американские биржи потеряли более 1 трлн долларов, а стоимость криптовалют снизилась на 7 %.\n",
      "Обвинение в краже технологий\n",
      "29 января 2025 года стало известно, что Microsoft и принадлежащая ей OpenAI начали расследование против DeepSeek. Компании утверждают, что китайский стартап несанкционированно обучал свои модели на данных, сгенерированных нейросетями OpenAI. По версии американской корпорации, DeepSeek не является независимой разработкой, а создана методом дистилляции разработанных и запатентованных OpenAI моделей.\n",
      "Санкции США как катализатор инноваций\n",
      "Известно, что для обучения моделей DeepSeek использовал закупленные ещё в 2021 году — то есть до введения США ограничений на поставки чипов в Китай — видеокарты Nvidia A100. По различным оценкам, в распоряжении стартапа DeepSeek находится около 10 тыс. штук таких видеокарт (некоторые западные эксперты считают, что компания аккумулировала еще около 40 тыс. урезанных для соблюдения экспортных рестрикций видеокарт Nvidia H800 ), в несколько раз меньше, чем у OpenAI или Llama (каждая компания имеет около 300 тыс. видеокарт). Таким образом, вероятно, ограниченные вычислительные мощности лишь подтолкнули DeepSeek к поиску перечисленных выше инновационных архитектурных решений, которые компенсировали этот недостаток.\n",
      "Критика\n",
      "Дарио Амодей, создатель компании Anthropic, разрабатывающей большую серию языковых моделей Claude AI, считает, что китайские разработчики скорее громко заявили о себе, нежели добились выдающихся результатов. По мнению Амодея, DeepSeek создал модель, близкую к американским моделям 7-10 месячной давности, потратив на это меньше средств, но в рамках обычного тренда снижения затрат; плюс китайский стартап имел доступ к серьезным ресурсам. Амодей отметил, что DeepSeek показал хорошие результаты, но не стоит называть это революцией, так как снижение затрат соответствует обычному тренду, и общие затраты DeepSeek сопоставимы с расходами американских ИИ-лабораторий, Кроме того, DeepSeek-V3 более инновационна, чем нашумевший DeepSeek-R1.\n",
      "Примечания\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff05d247-245e-4a6a-8353-ace5a2ec460c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('h1',\n",
       "  'DeepSeek (языковая модель)'): '2 ноября 2023 (DeepSeek Coder)\\n\\nDeepSeek — семейство языковых моделей с открытым исходным кодом, разрабатываемое одноимённой китайской лабораторией искусственного интеллекта. Новейшие версии большой языковой модели с чат-ботом — DeepSeek-V3 и «думающая» DeepSeek-R1\\xa0для решения логических задач — демонстрируют равные или лучшие показатели в обработке естественных языков, математических задачах и программировании по сравнению с разработками конкурентов ( GPT-4о или Open AI-o1 соответственно, а также аналогичные модели от других западных компаний) [1] [2] [3] [4].\\n\\nПри этом, по утверждению DeepSeek, их модели требуют на порядок меньше ресурсов для обучения и поддержания работы, а также не ограничивают доступ для пользователей из России и любых других стран мира [2] [5]. Кроме того, стоимость использования моделей в среднем на 96\\xa0% ниже, чем у западных конкурентов [6] [7].\\n\\nВыход на рынок модели DeepSeek-R1 20 января 2025\\xa0года вызвал масштабный обвал рыночных котировок технологических компаний США (как работающих в сфере искусственного интеллекта, так и производителей вычислительного оборудования), а также спровоцировал дискуссию об обоснованности многомиллиардных вложений в западную ИИ-инфраструктуру, завышенных требованиях к оборудованию и его стоимости [8] [9].',\n",
       " ('h2',\n",
       "  'История развития'): 'DeepSeek был выделен в самостоятельный стартап из ИИ-направления китайского хедж-фонда High-Flyer, основанного в 2015\\xa0году китайским миллиардером и энтузиастом в сфере ИИ Ляном Вэньфаном. В июле 2024 года он заявлял об ограниченности современных генеративных моделей, а задачей новой компании обозначал достижение общего искусственного интеллекта (AGI) [10]. В ноябре 2024\\xa0года глава компании ещё раз подтвердил свою приверженность идее создания продуктов с открытым исходным кодом и отсутствии планов по какому-либо дальнейшему расширению платных услуг DeepSeek [11].',\n",
       " ('h3',\n",
       "  'Условия распространения продуктов DeepSeek'): 'В отличие от ведущих западных компаний в области искусственного интеллекта ( OpenAI, Meta или Anthropic ) все языковые модели DeepSeek с самого начала распространялись под свободной лицензией MIT. Комментируя успех DeepSeek V3 и DeepSeek R-1 в январе 2025\\xa0года главный специалист Meta по искусственному интеллекту Ян Лекун заявил, что «в сфере ИИ модели с открытым исходным кодом доказали своё превосходство над проприетарными решениями» [12].',\n",
       " ('h2',\n",
       "  'Релизы'): 'С ноября 2023\\xa0года DeepSeek представила 3 генерации больших языковых моделей.',\n",
       " ('h3',\n",
       "  'DeepSeek Coder и DeepSeek LLM'): 'Первая нейросеть от DeepSeek Coder была представлена 2 ноября 2023 года. 29 ноября вышла первая универсальная большая языковая модель DeepSeek LLM с 67\\xa0млрд параметров, которая на тот момент превосходила возможности LLama 2 и приближалась к GPT-4 [13], однако, по некоторым данным, имела проблемы с масштабируемостью и вычислительной эффективностью [13]. Тогда же был впервые представлен интеллектуальный чат-бот, работающий на основе DeepSeek LLM.\\n\\nВсего было разработано 8 вариантов первой модели: четыре стандартных предобученных (Base) и четыре прошедших тонкую настройку набором инструкций (Instruct). Все они опирались на трансформерную архитектуру, использующую механизм « внимания », схожую с той что используют модели Llama.',\n",
       " ('h3',\n",
       "  'DeepSeek-V2'): 'В мае 2024\\xa0года DeepSeek выпустила вторую версию языковой модели в четырёх вариантах: стандартная (V2), уменьшенная (V2-Lite), а также стандартный чат-бот (V2-Chat) и его уменьшенная версия (V2-Chat-Lite).\\n\\nКоличество параметров в старших моделях возросло до 236\\xa0млрд; предобучение проводилось на 8,1\\xa0трлн токенов, а максимальная длина контекста возросла до 128\\xa0тыс. токенов.\\n\\nМодель претерпела значительные архитектурные изменения по сравнению с первой версией: в ней применён инновационный метод машинного обучения на основе низкоранговой аппроксимации Multi-head Learning Attention ( MLA, с англ. — «Обучение со множественным вниманием») [16], позволивший многократно уменьшить стоимость и время обучения модели [16] [17]. Кроме того компания разработала и внедрила в модель доработанный принцип Mixture of Experts ( MoE, с англ.\\xa0— « смесь экспертов »): при таком подходе модель состоит из большого числа подсетей, каждая из которых отвечает за свою специализированную область знаний и подключается к поиску ответа только по необходимости [18] [19].\\n\\nКомбинация MLA и MoE позволила установить стоимость обработки миллиона токенов в 2 юаня, в то время как у ChatGPT она равнялась 2,5 долларам за аналогичный объём информации [20]. При этом DeepSeek-V2 демонстрировала конкурентоспособное качество работы: лаборатории искусственного интеллекта при университете Ватерлоо поместила на 7 место в рейтинге лучших больших языковых моделей [21].',\n",
       " ('h3',\n",
       "  'DeepSeek-V3'): 'В декабре 2024 года в открытый доступ вышла третья генерация модели в двух вариантах: стандартная (V3-Base) и чат-бота (V3), содержащая 671\\xa0млрд параметров и проходившая обучение на 14,8\\xa0трлн токенов. Нейросеть, построенная на апробированной в DeepSeek-V2 архитектуре, способна анализировать и пересказывать тексты, выделяя главное, делать переводы, а также решать математические задачи и писать программы [2] на одном уровне с наиболее продвинутыми моделями от OpenAI, Meta или Anthropic: так DeepSeek-V3 превосходит в тестах Llama 3.1 Qwen 2.5 [1] и соответствует уровню GPT-4о и Claude 3.5 Sonnet [2] [3].\\n\\nПредставленное DeepSeek бесплатное приложение\\xa0— чат-бот «DeepSeek\\xa0— AI Assistant»\\xa0— к концу января 2025 стало самым скачиваемым в мире, а также обошло ChatGPT в рейтинге самых высокооценённых бесплатных приложений в США [22]. Кроме того DeepSeek не поддерживает политику санкций и никак не ограничивает доступ к своей модели, предоставляя равные возможности для пользователей из всех стран мира\\xa0— в том числе России [2].\\n\\nПри этом, по утверждению разработчиков модели, время её обучения составило всего лишь 55 дней на массиве из около 2000 урезанных для соблюдения требований экспортного контроля видеокарт Nvidia; таким образом стоимость обучения составила всего 5,5\\xa0млн долларов, как минимум на порядок раз ниже, чем Llama или GPT-4o [7] [23].',\n",
       " ('h4',\n",
       "  'Особенности модели'): 'Таких показателей эффективности удалось добиться благодаря особенностям архитектуры DeepSeek:',\n",
       " ('h3',\n",
       "  'DeepSeek-R1'): '20 января 2025\\xa0года DeepSeek представил свою думающую модель DeepSeek R1 для решения логических и математических задач с производительностью в математических тестах AIME и MATH на уровне флагманского решения Open AI-o1 [4]. Отличительная особенность этой модели заключается в пошаговой генерации ответов, повторяющей процесс мышления у человека [25] [26] [27]. При её разработке компания использовала новый, более эффективный подход к моделированию вознаграждения при обучении с подкреплением [28].',\n",
       " ('h4',\n",
       "  'Падение рынков в январе 2025\\xa0года'): 'Появление модели, которая требует многократно меньших затрат для обучения и использования, привела к обрушению котировок технологических компаний 28 января 2025\\xa0года. Так главный мировой поставщик оборудования для обучения нейросетей Nvidia потерял свыше 600\\xa0млрд долларов или почти 18\\xa0% капитализации, что стало крупнейшим обвалом в истории фондового рынка [29].\\n\\nСущественные потери понесли практически все связанные с сектором ИИ компании\\xa0— Nebius Аркадия Воложа (-4\\xa0%) Broadcom (-17,3\\xa0%), AMD (-8\\xa0%), Palantir (-7\\xa0%), Microsoft (-3\\xa0%), а также основные поставщики электроэнергии для дата-центров Constellation Energy (-21\\xa0%) и Vistra (-29\\xa0%). Индекс NASDAQ просел на 3,5\\xa0%, а S&P на 1,8\\xa0%. Суммарно же американские биржи потеряли более 1\\xa0трлн долларов [29], а стоимость криптовалют снизилась на 7\\xa0% [30].',\n",
       " ('h2',\n",
       "  'Обвинение в краже технологий'): '29 января 2025 года стало известно, что Microsoft и принадлежащая ей OpenAI начали расследование против DeepSeek. Компании утверждают, что китайский стартап несанкционированно обучал свои модели на данных, сгенерированных нейросетями OpenAI [31]. По версии американской корпорации, DeepSeek не является независимой разработкой, а создана методом дистилляции разработанных и запатентованных OpenAI моделей [32].',\n",
       " ('h3',\n",
       "  'Санкции США как катализатор инноваций'): 'Известно, что для обучения моделей DeepSeek использовал закупленные ещё в 2021\\xa0году\\xa0— то есть до введения США ограничений на поставки чипов в Китай\\xa0— видеокарты Nvidia A100. По различным оценкам, в распоряжении стартапа DeepSeek находится около 10\\xa0тыс. штук таких видеокарт [7] (некоторые западные эксперты считают, что компания аккумулировала еще около 40\\xa0тыс. урезанных для соблюдения экспортных рестрикций видеокарт Nvidia H800 [33] [34] ), в несколько раз меньше, чем у OpenAI или Llama (каждая компания имеет около 300\\xa0тыс. видеокарт) [35] [36]. Таким образом, вероятно, ограниченные вычислительные мощности лишь подтолкнули DeepSeek к поиску перечисленных выше инновационных архитектурных решений, которые компенсировали этот недостаток [37].',\n",
       " ('h2',\n",
       "  'Критика'): 'Дарио Амодей, создатель компании Anthropic, разрабатывающей большую серию языковых моделей Claude AI, считает, что китайские разработчики скорее громко заявили о себе, нежели добились выдающихся результатов. По мнению Амодея, DeepSeek создал модель, близкую к американским моделям 7-10 месячной давности, потратив на это меньше средств, но в рамках обычного тренда снижения затрат; плюс китайский стартап имел доступ к серьезным ресурсам. Амодей отметил, что DeepSeek показал хорошие результаты, но не стоит называть это революцией, так как снижение затрат соответствует обычному тренду, и общие затраты DeepSeek сопоставимы с расходами американских ИИ-лабораторий, Кроме того, DeepSeek-V3 более инновационна, чем нашумевший DeepSeek-R1 [38].',\n",
       " ('h2', 'Примечания'): ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc031a6-ec57-4350-b03a-e4d58cbe1bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:09<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "page.html_ref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d88b20-1a6b-4a54-8da0-16cf74b59bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:32<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "page.newspaper_ref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6055b33-1e76-4558-be3d-f8dab56ad39e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чат-бот китайской компании DeepSeek предлагает поиск данных в Сети без специальной подписки и доступен для российских пользователей. В конце января приложение с нейросетью стало самым скачиваемым на iPhone\n",
      "\n",
      "В ноябре 2024 года китайская компания DeepSeek открыла доступ к большой языковой модели DeepSeek V3. Разработчики утверждают, что чат-бот на базе модели способен конкурировать с ChatGPT, а основатели компании и вовсе нацелились на создание «сверхразумного» ИИ. Рассказываем, что представляет собой модель, какие она предлагает опции и как с ней работать.\n",
      "\n",
      "Что такое DeepSeek V3\n",
      "\n",
      "DeepSeek V3 — это большая языковая модель с открытым исходным кодом, которая содержит 671 млрд параметров и обучена на 14,8 трлн токенов. Она способна анализировать тексты, делать переводы и писать эссе, а также создавать код.\n",
      "\n",
      "Особенности модели кроются в ее архитектуре и методах обучения. Она использует:\n",
      "\n",
      "Архитектуру Multi-token Prediction (MTP). Это позволяет модели предсказывать несколько слов вместо одного, анализируя одновременно разные части предложения. Такой метод повышает точность работы модели и ее производительность;\n",
      "\n",
      "Mixture of Experts (MoE). Эта архитектура использует несколько специализированных и заранее обученных нейросетей-«экспертов» для анализа различных входных данных. Это позволяет ускорить обучение и повысить эффективность ИИ. DeepSeek V3 работает с 256 такими нейросетями, из которых восемь активируются для обработки каждого токена;\n",
      "\n",
      "Технологию Multi-head Latent Attention (MLA) — механизм внимания, который обычно используется в больших языковых моделях и помогает им идентифицировать наиболее важные части предложения. MLA позволяет извлекать ключевые детали из фрагмента текста несколько раз, а не только один. Это означает, что ИИ с меньшей вероятностью упустит важную информацию.\n",
      "\n",
      "Благодаря этим особенностям модель потребовала всего 2,788 млн часов или два месяца работы графических процессоров Nvidia H800 для обучения. Затраты на него составили $5,5 млн. Для сравнения — OpenAI потратила на обучение GPT $78 млн.\n",
      "\n",
      "Разработчики утверждают, что в тестах нейросеть превзошла GPT-4о от OpenAI, Llama 3 от Meta (признана экстремистской, запрещена в России) и Claude 3.5 Sonnet от Anthropic в задачах программирования и обработки текста.\n",
      "\n",
      "Результаты тестов DeepSeek V3 и конкурентов (Фото: github.com)\n",
      "\n",
      "Главная особенность новой модели — это полностью открытый код, который позволяет разработчикам не только использовать технологию для коммерческих целей, но и адаптировать ее для решения различных задач в сфере искусственного интеллекта.\n",
      "\n",
      "Возможности DeepSeek V3\n",
      "\n",
      "Модель предлагает контекстное окно в 128 тыс. токенов, как и GPT-4o, что позволяет ей анализировать до 300 страниц текста. Она способна:\n",
      "\n",
      "генерировать тексты разных объемов и в разных жанрах;\n",
      "\n",
      "искать информацию в интернете;\n",
      "\n",
      "расшифровывать диаграммы и объяснять картинки;\n",
      "\n",
      "писать код, корректно форматировать его и решать сложные задачи по программированию на языках C++, Go, Java, JavaScript, Python и Rust. Модель успешно интегрируется с редакторами кода;\n",
      "\n",
      "рассуждать подобно GPT-o1 и o1-mini в режиме DeepThink.\n",
      "\n",
      "DeepSeek V3 предлагает мультиязычность высокого уровня, а ее глубокое понимание китайского и английского позволяет работать с текстами без потери качества переводенного текста и смысла. Модель также поддерживает русский язык.\n",
      "\n",
      "Минус нейросети в том, что пока она не позволяет анализировать материалы по ссылкам, а поддерживает только загрузки или выдержки из текстов.\n",
      "\n",
      "Как пользоваться DeepSeek V3 в России\n",
      "\n",
      "DeepSeek предлагает несколько вариантов доступа, в том числе через открытые модели на Hugging Face, бесплатную версию на собственном сайте с контекстным окном до 32 тыс. токенов, API для коммерческого использования, локальное развертывание и мобильные приложения для iOS и Android. При выходе DeepSeek-R1 пользователям было доступно 50 бесплатных запросов в день, сейчас ограничения на сайте не указаны.\n",
      "\n",
      "Как воспользоваться бесплатной версией на сайте\n",
      "\n",
      "Пользователи в России могут получить доступ к DeepSeek V3 через сайт. Для этого нужно:\n",
      "\n",
      "перейти на сайт DeepSeek, нажать Start Now;\n",
      "\n",
      "пройти регистрацию (можно с помощью аккаунта Google);\n",
      "\n",
      "откроется диалоговое окно с чат-ботом. В нем, помимо обычного ответа, можно выбрать опцию DeepThink для рассуждений или Search для поиска данных в Интернете. Также для анализа можно прикрепить до 50 файлов разного формата размером до 100 МБ каждый.\n",
      "\n",
      "Как воспользоваться мобильным приложением\n",
      "\n",
      "В декабре 2024 года DeepSeek выпустила бесплатное мобильное приложение для своего чат-бота. Оно доступно в российских магазинах App Store и Google Play. Также приложение можно скачать с сайта разработчиков, выбрав Get DeepSeek App и просканировав QR-код.\n",
      "\n",
      "После установки и открытия приложения нужно нажать Agree, чтобы согласиться с условиями его использования, пройти регистрацию, после чего появится диалоговое окно с чат-ботом.\n",
      "\n",
      "Интерфейс мобильного приложения DeepSeek (Фото: deepseek.com)\n",
      "\n",
      "Примеры использования DeepSeek\n",
      "\n",
      "Протестируем возможности модели в задаче генерации текста — попросим ее написать статью о трендах развития нейросетей в 2025 году. Чат-бот выдал структурированный материал на 4,5 тыс. знаков.\n",
      "\n",
      "Статья про нейросети от DeepSeek V3 (Фото: deepseek.com)\n",
      "\n",
      "А теперь попросим нейросеть подобрать специализированные англоязычные источники для этого материала. DeepSeek V3 предложила выборку из 41 ресурса, в том числе сайтов научных работ arXiv, IEEEXplore и Nature со ссылками и пояснениями.\n",
      "\n",
      "Выборка сайтов от DeepSeek V3 (Фото: deepseek.com)\n",
      "\n",
      "Предложим чат-боту порассуждать — отправим ему рассказ «Лигейя» Эдгара Аллана По и попросим объяснить, в чем смысл предисловия.\n",
      "\n",
      "DeepSeek V3 объясняет суть предисловия к рассказу в режиме DeepThink (Фото: deepseek.com)\n",
      "\n",
      "Вернемся к актуальным событиям и попросим чат-бота подготовить подборку десяти ресурсов с афишами культурных мероприятий Москвы. Нейросеть подготовила выборку не только российских, но и англоязычных сайтов.\n",
      "\n",
      "Выборка афиш от DeepSeek V3 (Фото: deepseek.com)\n",
      "\n",
      "А теперь попросим чат-бота отобрать самые интересные театральные премьеры зимнего сезона.\n",
      "\n",
      "Подборка театральных премьер от DeepSeek V3 (Фото: deepseek.com)\n",
      "\n",
      "Испытаем возможности DeepSeek V3 в анализе больших текстов. Попросим ИИ кратко пересказать сюжет «Маленького принца» Антуана де Сент-Экзюпери объемом 112 страниц. DeepSeek V3 в ответ не только передает сюжет, но и представляет главных героев, описывает основные темы рассказа и кратко резюмирует его.\n",
      "\n",
      "Анализ «Маленького принца» от DeepSeek V3 (Фото: deepseek.com)\n",
      "\n",
      "Напоследок попросим нейросеть решить задачу по программированию, написав алгоритм для поиска минимального числа линий, которые необходимы для построения диаграммы. В ответ DeepSeek V3 не только выдает искомый алгоритм на языке Python, но и объясняет ход решения задачи, а также возможные сложности.\n",
      "\n",
      "\n",
      "Компания DeepSeek, поддерживаемая китайским хедж-фондом High-Flyer, представила новую языковую модель DeepSeek-V3, которая продемонстрировала впечатляющие результаты в работе с кодом.\n",
      "\n",
      "Архитектурные особенности\n",
      "\n",
      "DeepSeek-V3 представляет собой значительный шаг вперед по сравнению со своим предшественником. Модель имеет 685 миллиардов параметров. В основе архитектуры лежит подход Mixture of Experts (MoE) с 256 экспертами, из которых 8 активируются для каждого токена.\n",
      "\n",
      "По сравнению с предыдущей версией, DeepSeek-V3 получила существенные улучшения во всех ключевых параметрах. Новая версия может обрабатывать больше информации за один раз, имеет заметно расширенный словарный запас и значительно более мощную внутреннюю архитектуру. Все эти улучшения направлены на то, чтобы модель лучше понимала контекст и генерировала более качественные ответы.\n",
      "\n",
      "Впечатляющие результаты в Aider Polyglot Benchmark\n",
      "\n",
      "Особого внимания заслуживают результаты DeepSeek-V3 в тесте Aider Polyglot — специализированном бенчмарке для оценки способностей языковых моделей в работе с кодом на различных языках программирования. Тест включает 225 сложнейших задач с платформы Exercism по программированию на C++, Go, Java, JavaScript, Python и Rust.\n",
      "\n",
      "В данном тестировании DeepSeek-V3 показала результат в 48.4% успешно решенных задач, заняв второе место в общем рейтинге. Модель уступила только o1-2024-12-17 (61.7%), но превзошла такие известные модели как Claude-3-5-sonnet-20241022 (45.3%) и Gemini-exp-1206 (38.2%).\n",
      "\n",
      "Результаты DeepSeek-V3 в тесте Aider Polyglot\n",
      "\n",
      "Важной особенностью теста является не только процент решенных задач, но и способность модели корректно форматировать изменения в коде. DeepSeek-V3 показала впечатляющий результат в 98.7% правильного форматирования изменений.\n",
      "\n",
      "Мультимодальные возможности\n",
      "\n",
      "Помимо впечатляющих результатов в работе с кодом, DeepSeek-V3 хорошо справляется с другими задачами: читает диаграммы, работает с научными текстами и сайтами, понимает картинки и помогает создавать разные тексты. Модель можно попробовать на сайте chat.deepseek.com.\n",
      "\n",
      "Интересная особенность\n",
      "\n",
      "Ответы модели\n",
      "\n",
      "Любопытной деталью является то, как модель представляет себя на разных языках. В англоязычной версии чата она называет себя \"DeepSeek-V3, AI assistant created exclusively by the Chinese Company DeepSeek\", в то время как в русскоязычной версии она представляется как \"языковая модель OpenAI, основанная на архитектуре GPT-4\". Такое различие в самоидентификации на разных языках является необычным и заслуживает внимания при оценке возможностей модели.\n",
      "\n",
      "\n",
      "Вчера, 20 января, китайская лаборатория DeepSeek сделала нам всем настоящий подарок, открыв доступ к новой reasoning-модели R1, которая уже штурмует вершины ML-бенчмарков.\n",
      "\n",
      "R1 – не просто еще одна рассуждающая модель: это первая бесплатная моделька с открытыми весами, которая добивается таких результатов. На математическом бенчмарке AIME 2024 она достигает 79.8%, обогнав даже обновленную версию o1 с ее 79.2%, не говоря уже об o1-mini (63.6%). В кодинге R1 тоже хороша. Например, на Codeforces ее результат – 96.3%, что практически недостижимо для большинства людей.\n",
      "\n",
      "Моделью уже можно воспользоваться в чате chat.deepseek.com/. Доступно 50 сообщений в день, VPN не требуется. И самое прекрасное: кроме весов и кода DeepSeek выложили замечательный тех.отчет, в котором подробно описали, как им удалось обучить такую мощную модель. Сейчас мы разберем его по полочкам.\n",
      "\n",
      "Итак, надо сказать, что на самом деле DeepSeek представили не одну модель, а целых 8: саму R1, ее младшую сестренку R1-Zero и 6 дистиллированных, то есть уменьшенных, моделей. К ним еще вернемся, о пока начнем с R1 Zero.\n",
      "\n",
      "Несмотря на то, что R1 – умнейшая из них, самая интересная с точки зрения техники исполнения, пожалуй, именно R1-Zero. Ноль в названии фигурирует не просто так. Дело в том, что R1-Zero была обучена вообще без использование каких-либо размеченных людьми данных. Учитывая ее результаты, это просто поражает. Большинство LLM обучаются в три этапа:\n",
      "\n",
      "Претрейн на большом количестве текста. На этом этапе модель выучивается понимать текст и связно его генерировать, а также запоминает факты и набирается общих знаний о мире и языке. Файнтюнинг. Обучение на размеченных данных вида <вопрос-идеальный ответ>. Нужно для того, чтобы модель научилась лучше следовать инструкциям. Именно на этом этапе обычно наблюдается значительное улучшение метрик, потому что модель начинает не просто генерировать какой-то связный текст, а делать это именно так, чтобы хорошо выполнять поставленные задачи. Обучение с подкреплением. Здесь модель дорабатывают, чтобы она лучше соответствовала пользовательским ожиданиям и была максимально полезной. Например, её учат более корректно и чётко формулировать ответы или избегать вредоносного или неуместного контента.\n",
      "\n",
      "Теперь представьте, что R1-Zero пропустила второй этап с разметкой полностью. Это значит никакого файнтюнинга на размеченных данных \"вопрос-ответ\", только базовая предобученная модель DeepSeek-V3-Base и Reinforcement Learning, который тоже реализован без использования разметки. В качестве алгоритма RL традиционно для DeepSeek применяется GRPO, улучшенная версия PPO. Отдельно поощряется формат, в котором модель помещает свои рассуждения внутри тегов <think> и </think>.\n",
      "\n",
      "Лирическое отступление: если вы хотите лучше понять, как устроено обучение с подкреплением, для чего оно нужно, и как работают алгоритмы PPO и GRPO в частности, прочитайте нашу большую статью \"DeepSeekMath или как научить LLM решать математические задачи\". А если хотите каждый день читать что-нибудь интересное про ML, то приглашаем вас в наш тг-канал Data Secrets. Там мы (а мы – это команда действующих ML-инженеров) каждый день наблюдаем за повесткой, публикуем разборы свежих статей и релизов и делимся прикладными материалами. А еще наше большое сообщество всегда радо новым специалистам и энтузиастам :)\n",
      "\n",
      "Вернемся к R1-Zero. Несмотря на то, что для нее полностью пропустили этап файнтюнинга, ее результаты поражают. Во-первых, после нескольких тысяч итераций RL точность на математическом тесте AIME скакнула с 15.6% в базовой модели до 71.0% (вау!), и уже здесь DeepSeek опередили o1-mini.\n",
      "\n",
      "Во-вторых, посмотрите на график ниже. Он показывает, как росла длина ответом модели по мере тренировки. Получается, что R1-Zero по мере обучения буквально учится рассуждать дольше и за счет этого получать лучшие результаты!\n",
      "\n",
      "А еще модель даже самостоятельно научилась находить и выделять \"ага-моменты\", то есть постигать какие-то ключевые инсайты, необходимые для решения, или, иными словами \"догадываться\" до чего-то только благодаря тому, что хорошо подумала. Вот такая вот сила обучения с подкреплением.\n",
      "\n",
      "Напоминаем, что все это – вообще без разметки. Неплохо, да?\n",
      "\n",
      "Однако DeepSeek пошли дальше, и для повышения качества решили все-таки добавить подготовленные вручную данные в процесс обучения. Так появилась R1. Для нее, в целом, пайплайн обучения и даже алгоритм RL остаются такими же, с небольшой разницей. Для R1-Zero в RL мы использовали rule-based rewards, когда ответы проверяются только самой системой, без внешних разметок. Например, если задача на программирование — в компиляторе проверяется успешное выполнение тестов на написанном коде.\n",
      "\n",
      "И хотя точность таким образом получается приличная, сами ответы читать сложно: в них смешиваются языки, нет форматирования и тд. Поэтому R1 в начале дообучили на специально подготовленных готовых цепочках рассуждений. Данные брали из DeepSeek-R1-Zero и, видимо, o1, а затем отбирали + улучшали вручную.\n",
      "\n",
      "Эти же данные затем также применяют в RL. А еще ключевую роль здесь сыграл подход rejection sampling: во время обучения модель генерировала множество вариантов ответов, из которых отбирались лучшие. Эти отборные ответы возвращались в обучающую выборку, чтобы постепенно повышать качество как самих рассуждений, так и итогового результата. Получился отлично функционирующий цикл вида \"генерация → отбор → дообучение\".\n",
      "\n",
      "Относительно R1-Zero метрики тут еще более впечатляющие, и уже тягаются с полноценной o1. Но есть интересный факт: когда на этапе RL для R1 ввели правило \"доля таргетного языка в ответе должна быть больше 0.95\", качество немножко просело.\n",
      "\n",
      "Наконец, дистиллированные модели. Они нужны, потому что сама по себе R1 огромная – 685B параметров. Это значит, что ее почти невозможно запустить локально, если вы не GPU-rich. А локальный запуск очень важен, особенно для исследователей или тех, кто хочет использовать R1 безлимитно или оффлайн на своей машине. Поэтому лаборатория DeepSeek позаботились о том, чтобы у нас были мини-версии ее ризонера.\n",
      "\n",
      "Вообще, дистилляция – это когда мы переносим знания из большой сложной модели (учителя) в меньшую и более простую модель (ученика). Цель в том, чтобы создать модель пободрее и поменьше, но такую, которая сохраняет большую часть перформанса исходной. В данном случае в качестве учителя выступает R1, а в качестве учеников – открытые Qwen и Llama.\n",
      "\n",
      "Процесс был примерно такой: из R1 насемплировали 800,000 примеров, на которых ванильно зафайнтюнили учеников. Тут, к слову, вообще не использовался RL, но в статье написано, что ученые хотят попробовать его применить.\n",
      "\n",
      "В итоге кроме R1-Zero и R1 у нас есть целых 6 их падавана на 1.5B, 7B, 14B, 32B, 8B, 70B. При этом 32 и 70 на уровне o1-mini, а 1.5B аутперформит GPT-4o и Сlaude Sonnet!\n",
      "\n",
      "В общем, очень крутая работа получилась у DeepSeek. Они не только еще раз показали нам силу RL, но и напомнили, что передовые модели ИИ не обязательно должны скрываться за семью печатями, а могут оставаться открытыми и доступными.\n",
      "\n",
      "Полностью тех.отчет читайте здесь, пробуйте R1 здесь, а веса дистилляций качайте вот тут. Высоких вам метрик!\n",
      "\n",
      "\n",
      "Вчера, 20 января, китайская лаборатория DeepSeek сделала нам всем настоящий подарок, открыв доступ к новой reasoning-модели R1, которая уже штурмует вершины ML-бенчмарков.\n",
      "\n",
      "R1 – не просто еще одна рассуждающая модель: это первая бесплатная моделька с открытыми весами, которая добивается таких результатов. На математическом бенчмарке AIME 2024 она достигает 79.8%, обогнав даже обновленную версию o1 с ее 79.2%, не говоря уже об o1-mini (63.6%). В кодинге R1 тоже хороша. Например, на Codeforces ее результат – 96.3%, что практически недостижимо для большинства людей.\n",
      "\n",
      "Моделью уже можно воспользоваться в чате chat.deepseek.com/. Доступно 50 сообщений в день, VPN не требуется. И самое прекрасное: кроме весов и кода DeepSeek выложили замечательный тех.отчет, в котором подробно описали, как им удалось обучить такую мощную модель. Сейчас мы разберем его по полочкам.\n",
      "\n",
      "Итак, надо сказать, что на самом деле DeepSeek представили не одну модель, а целых 8: саму R1, ее младшую сестренку R1-Zero и 6 дистиллированных, то есть уменьшенных, моделей. К ним еще вернемся, о пока начнем с R1 Zero.\n",
      "\n",
      "Несмотря на то, что R1 – умнейшая из них, самая интересная с точки зрения техники исполнения, пожалуй, именно R1-Zero. Ноль в названии фигурирует не просто так. Дело в том, что R1-Zero была обучена вообще без использование каких-либо размеченных людьми данных. Учитывая ее результаты, это просто поражает. Большинство LLM обучаются в три этапа:\n",
      "\n",
      "Претрейн на большом количестве текста. На этом этапе модель выучивается понимать текст и связно его генерировать, а также запоминает факты и набирается общих знаний о мире и языке. Файнтюнинг. Обучение на размеченных данных вида <вопрос-идеальный ответ>. Нужно для того, чтобы модель научилась лучше следовать инструкциям. Именно на этом этапе обычно наблюдается значительное улучшение метрик, потому что модель начинает не просто генерировать какой-то связный текст, а делать это именно так, чтобы хорошо выполнять поставленные задачи. Обучение с подкреплением. Здесь модель дорабатывают, чтобы она лучше соответствовала пользовательским ожиданиям и была максимально полезной. Например, её учат более корректно и чётко формулировать ответы или избегать вредоносного или неуместного контента.\n",
      "\n",
      "Теперь представьте, что R1-Zero пропустила второй этап с разметкой полностью. Это значит никакого файнтюнинга на размеченных данных \"вопрос-ответ\", только базовая предобученная модель DeepSeek-V3-Base и Reinforcement Learning, который тоже реализован без использования разметки. В качестве алгоритма RL традиционно для DeepSeek применяется GRPO, улучшенная версия PPO. Отдельно поощряется формат, в котором модель помещает свои рассуждения внутри тегов <think> и </think>.\n",
      "\n",
      "Лирическое отступление: если вы хотите лучше понять, как устроено обучение с подкреплением, для чего оно нужно, и как работают алгоритмы PPO и GRPO в частности, прочитайте нашу большую статью \"DeepSeekMath или как научить LLM решать математические задачи\". А если хотите каждый день читать что-нибудь интересное про ML, то приглашаем вас в наш тг-канал Data Secrets. Там мы (а мы – это команда действующих ML-инженеров) каждый день наблюдаем за повесткой, публикуем разборы свежих статей и релизов и делимся прикладными материалами. А еще наше большое сообщество всегда радо новым специалистам и энтузиастам :)\n",
      "\n",
      "Вернемся к R1-Zero. Несмотря на то, что для нее полностью пропустили этап файнтюнинга, ее результаты поражают. Во-первых, после нескольких тысяч итераций RL точность на математическом тесте AIME скакнула с 15.6% в базовой модели до 71.0% (вау!), и уже здесь DeepSeek опередили o1-mini.\n",
      "\n",
      "Во-вторых, посмотрите на график ниже. Он показывает, как росла длина ответом модели по мере тренировки. Получается, что R1-Zero по мере обучения буквально учится рассуждать дольше и за счет этого получать лучшие результаты!\n",
      "\n",
      "А еще модель даже самостоятельно научилась находить и выделять \"ага-моменты\", то есть постигать какие-то ключевые инсайты, необходимые для решения, или, иными словами \"догадываться\" до чего-то только благодаря тому, что хорошо подумала. Вот такая вот сила обучения с подкреплением.\n",
      "\n",
      "Напоминаем, что все это – вообще без разметки. Неплохо, да?\n",
      "\n",
      "Однако DeepSeek пошли дальше, и для повышения качества решили все-таки добавить подготовленные вручную данные в процесс обучения. Так появилась R1. Для нее, в целом, пайплайн обучения и даже алгоритм RL остаются такими же, с небольшой разницей. Для R1-Zero в RL мы использовали rule-based rewards, когда ответы проверяются только самой системой, без внешних разметок. Например, если задача на программирование — в компиляторе проверяется успешное выполнение тестов на написанном коде.\n",
      "\n",
      "И хотя точность таким образом получается приличная, сами ответы читать сложно: в них смешиваются языки, нет форматирования и тд. Поэтому R1 в начале дообучили на специально подготовленных готовых цепочках рассуждений. Данные брали из DeepSeek-R1-Zero и, видимо, o1, а затем отбирали + улучшали вручную.\n",
      "\n",
      "Эти же данные затем также применяют в RL. А еще ключевую роль здесь сыграл подход rejection sampling: во время обучения модель генерировала множество вариантов ответов, из которых отбирались лучшие. Эти отборные ответы возвращались в обучающую выборку, чтобы постепенно повышать качество как самих рассуждений, так и итогового результата. Получился отлично функционирующий цикл вида \"генерация → отбор → дообучение\".\n",
      "\n",
      "Относительно R1-Zero метрики тут еще более впечатляющие, и уже тягаются с полноценной o1. Но есть интересный факт: когда на этапе RL для R1 ввели правило \"доля таргетного языка в ответе должна быть больше 0.95\", качество немножко просело.\n",
      "\n",
      "Наконец, дистиллированные модели. Они нужны, потому что сама по себе R1 огромная – 685B параметров. Это значит, что ее почти невозможно запустить локально, если вы не GPU-rich. А локальный запуск очень важен, особенно для исследователей или тех, кто хочет использовать R1 безлимитно или оффлайн на своей машине. Поэтому лаборатория DeepSeek позаботились о том, чтобы у нас были мини-версии ее ризонера.\n",
      "\n",
      "Вообще, дистилляция – это когда мы переносим знания из большой сложной модели (учителя) в меньшую и более простую модель (ученика). Цель в том, чтобы создать модель пободрее и поменьше, но такую, которая сохраняет большую часть перформанса исходной. В данном случае в качестве учителя выступает R1, а в качестве учеников – открытые Qwen и Llama.\n",
      "\n",
      "Процесс был примерно такой: из R1 насемплировали 800,000 примеров, на которых ванильно зафайнтюнили учеников. Тут, к слову, вообще не использовался RL, но в статье написано, что ученые хотят попробовать его применить.\n",
      "\n",
      "В итоге кроме R1-Zero и R1 у нас есть целых 6 их падавана на 1.5B, 7B, 14B, 32B, 8B, 70B. При этом 32 и 70 на уровне o1-mini, а 1.5B аутперформит GPT-4o и Сlaude Sonnet!\n",
      "\n",
      "В общем, очень крутая работа получилась у DeepSeek. Они не только еще раз показали нам силу RL, но и напомнили, что передовые модели ИИ не обязательно должны скрываться за семью печатями, а могут оставаться открытыми и доступными.\n",
      "\n",
      "Полностью тех.отчет читайте здесь, пробуйте R1 здесь, а веса дистилляций качайте вот тут. Высоких вам метрик!\n",
      "\n",
      "\n",
      "DeepSeek R1: LLM с открытым исходным кодом с производительностью наравне с моделью o1 от OpenAI\n",
      "\n",
      "В течение нескольких дней я потратил значительную часть своего времени на то, чтобы опробовать новый китайский ИИ-чатбот Deepseek R-1. За последние несколько дней он привлек к себе много внимания, и на то есть веские причины: чатбот действительно способный - иногда даже лучше, чем ChatGPT. И он дешевый. Очень дешевый.\n",
      "\n",
      "Несмотря на то, что он появился относительно недавно, он уже успел зарекомендовать себя в сфере ИИ как рассуждающая модель с открытым исходным кодом. По многим показателям производительность находится на одном уровне с моделью o1 от OpenAI, а стоимость постоянного использования чата и API значительно ниже, чем у конкурентов.\n",
      "\n",
      "Как человек, который любит пробовать новейшие ИИ-инструменты, я сразу же приступил к работе и пользовался Deepseek R-1 в течение нескольких дней. Удивительно, но он ни разу не завис, не тормозил и, что еще более удивительно, ни разу не попросил меня купить подписку и не сказал, что я превысил свой ежедневный лимит использования.\n",
      "\n",
      "Что такое Deepseek R-1?\n",
      "\n",
      "Deepseek R-1 - это новейшая рассуждающая модель от китайской ИИ-лаборатории DeepSeek. Она имеет полностью открытый исходный код, то есть любой желающий может взять базовую кодовую базу, адаптировать ее и даже доработать под свои нужды.\n",
      "\n",
      "С технической точки зрения Deepseek R-1 (или просто R1) базируется на большой базовой модели под названием DeepSeek-V3. Затем лаборатория усовершенствовала эту модель с помощью комбинации контролируемой тонкой настройки (SFT) на высококачественных данных с человеческими метками и обучения с подкреплением (RL).\n",
      "\n",
      "Также они представили такие вариации, как R1-Zero, которая обходится без данных тонкой настройки с человеческими метками и пытается научиться «рассуждать» исключительно с помощью RL и «вычислений в тестовом времени».\n",
      "\n",
      "Тот факт, что Deepseek обнародовала эти модели и подробный технический отчет, говорит о желании поделиться полученными знаниями, что делает их интригующим примером для других ИИ-лабораторий, которые держат внутренние исследования под строгим секретом.\n",
      "\n",
      "Что еще более удивительно, так это то, что эта модель на самом деле появилась как побочный проект, чтобы использовать дополнительные графические процессоры.\n",
      "\n",
      "DeepSeek R1: LLM с открытым исходным кодом с производительностью наравне с моделью o1 от OpenAI\n",
      "\n",
      "Если серьезно, то эти инженеры безумно хороши. Побочный проект, который теперь конкурирует с языковыми моделями стоимостью в миллиард долларов? Это не просто впечатляет, это гениально. Это тот вид изобретательности, который заставляет задуматься, подают ли в их кафетерии креатив на завтрак.\n",
      "\n",
      "Как Deepseek R-1 по сравнению с o1\n",
      "\n",
      "Итак, как же Deepseek R-1 конкурирует с o1 от OpenAI и другими ведущими моделями?\n",
      "\n",
      "Короткий ответ: очень хорошо.\n",
      "\n",
      "Собственные бенчмарки Deepseek показывают, что R1 и o1 находятся примерно на одном уровне во многих категориях, от математики (например, бенчмарк AIME) до задач программирования (например, Codeforces) и даже продвинутых наборов QA, таких как GPQA Diamond. На самом деле, разница в производительности часто составляет всего несколько процентных пунктов.\n",
      "\n",
      "DeepSeek R1: LLM с открытым исходным кодом с производительностью наравне с моделью o1 от OpenAI\n",
      "\n",
      "Другие крупные игроки, такие как Gemini 2.0 от Google и Claude 3.5 от Anthropic, также вступают в борьбу. Некоторые тесты показывают, что эти модели находятся на одном уровне производительности с o1, но преимущество R1 в стоимости и доступность открытого исходного кода делают ее сильным соперником.\n",
      "\n",
      "Если учесть, что цена токена R1 в 30 раз дешевле, чем у o1, многие разработчики и опытные пользователи обратили на нее пристальное внимание.\n",
      "\n",
      "Вот сравнение моделей DeepSeek-R1-Zero и OpenAI o1 в бенчмарках, связанных с рассуждениями.\n",
      "\n",
      "DeepSeek R1: LLM с открытым исходным кодом с производительностью наравне с моделью o1 от OpenAI\n",
      "\n",
      "Когда в бенчмарке AIME используется мажоритарное голосование, производительность DeepSeek-R1-Zero возрастает с 71,0 до 86,7 %, превышая показатели OpenAI-o1-0912.\n",
      "\n",
      "Подробнее о технических характеристиках DeepSeek R1 можно узнать здесь.\n",
      "\n",
      "R1 - это очень дешево\n",
      "\n",
      "За все время, что я пробовал чатбота на chat.deepseek.com, я ни разу не столкнулся с оплатой или лимитом использования. Возможно, у Deepseek есть скрытый лимит использования, но если это так, то я его не достиг.\n",
      "\n",
      "Кроме того, он совсем не тормозил. Многие ИИ-чаты могут снижать производительность, если вы пытаетесь слишком сильно их нагрузить или входите в систему в часы пик. R1 работал быстро от начала и до конца.\n",
      "\n",
      "Но предположим, что вы разработчик или основатель стартапа, желающий интегрировать большую языковую модель в свой продукт. Пока что модели OpenAI серии o1 или GPT стоят первыми в списке лучших вариантов, но стоимость API может быстро увеличиться. Согласно данным Deepseek, вы можете использовать API R1 за сумму, в разы меньшую, чем вы заплатите OpenAI.\n",
      "\n",
      "💰 $0,14 за миллион входных токенов (попадание в кэш)\n",
      "\n",
      "💰 $0,55 за миллион входных токенов (пропуск кэша)\n",
      "\n",
      "💰 $2,19 за миллион выходных токенов\n",
      "\n",
      "Токен вывода почти в 30 раз дешевле, чем токены вывода o1 стоимостью 60 долларов за миллион. Это огромное сокращение расходов для компаний, занимающихся крупномасштабными ИИ-операциями.\n",
      "\n",
      "Посмотрите на это визуальное сравнение моделей R1 от DeepSeek и OpenAI.\n",
      "\n",
      "DeepSeek R1: LLM с открытым исходным кодом с производительностью наравне с моделью o1 от OpenAI\n",
      "\n",
      "Время покажет, долго ли продержатся бесплатные предложения Deepseek для постоянных пользователей чата. Большой наплыв новых пользователей может создать большую нагрузку на серверы, и типичная бизнес-логика подскажет: «Теперь нужно монетизировать». Но пока R1 остается бесплатной для повседневного использования.\n",
      "\n",
      "Доступ к API DeepSeek R1\n",
      "\n",
      "Перед тем как выдать ответ, модель генерирует цепочку мыслей (CoT), чтобы повысить точность своих ответов. API позволяет пользователям получить доступ к этому содержимому CoT, что дает возможность просматривать, отображать и анализировать процесс рассуждений, лежащий в основе ответов модели.\n",
      "\n",
      "Вот пример на Python, показывающий, как использовать API для одно- и многораундовых бесед:\n",
      "\n",
      "from openai import OpenAI # Initialize the client client = OpenAI(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\") # Round 1 messages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}] response = client.chat.completions.create( model=\"deepseek-reasoner\", messages=messages ) # Access the reasoning and final answer reasoning_content = response.choices[0].message.reasoning_content content = response.choices[0].message.content # Print outputs print(\"Reasoning:\", reasoning_content) print(\"Answer:\", content) # Round 2 messages.append({'role': 'assistant', 'content': content}) messages.append({'role': 'user', 'content': \"How many Rs are there in the word 'strawberry'?\"}) response = client.chat.completions.create( model=\"deepseek-reasoner\", messages=messages ) # Access and print results for Round 2 reasoning_content = response.choices[0].message.reasoning_content content = response.choices[0].message.content print(\"Reasoning:\", reasoning_content) print(\"Answer:\", content)\n",
      "\n",
      "Прежде чем использовать DeepSeek-Reasoner, убедитесь, что у вас установлена последняя версия OpenAI SDK:\n",
      "\n",
      "pip3 install -U openai\n",
      "\n",
      "Вот параметры API:\n",
      "\n",
      "max_tokens (входной параметр) устанавливает максимальную длину финального ответа после генерации CoT. По умолчанию - 4 000 токенов, максимум - 8 000 токенов.\n",
      "\n",
      "reasoning_content (выходной параметр) - процесс рассуждения (CoT), доступный как часть выводимой структуры.\n",
      "\n",
      "content (выходной параметр) - окончательный ответ, выданный моделью.\n",
      "\n",
      "Что касается длины контекста, API поддерживает максимальную длину контекста в 64 000 токенов. Однако содержимое reasoning_content не учитывается в этом лимите, что позволяет проводить обширные рассуждения без ущерба для контекста.\n",
      "\n",
      "Следует иметь в виду несколько моментов:\n",
      "\n",
      "Чтобы обеспечить бесперебойную работу API, при отправке нового запроса удалите поле reasoning_content из входных сообщений.\n",
      "\n",
      "CoT позволяет пользователям получить более глубокое представление о процессе рассуждений модели, что делает его ценным инструментом для исследований и анализа.\n",
      "\n",
      "В целом DeepSeek R-1 - мощная, быстрая и дешевая - это качества, которые могут изменить рынок и вызвать появление совершенно новых видов продуктов на базе ИИ. Помимо непосредственного восторга от появления нового способного чатбота, появление R1 подчеркивает более глубокий и сложный сюжет в ИИ-сообществе: гонка за ИИ и меняющийся баланс между Китаем и США.\n",
      "\n",
      "Недавно Сэм Альтман столкнулся с обратной реакцией. Он, как известно, колеблется между тем, чтобы превозносить AGI как следующее большое экзистенциальное событие, и тем, чтобы призвать всех успокоиться, потому что AGI не появится так быстро, как мы думаем.\n",
      "\n",
      "Однако темпы прорыва ИИ, похоже, не позволяют успокоиться. С выходом Deepseek R-1 мы видим, насколько непредсказуема эта область. Не только «большая тройка» (OpenAI, Google, Anthropic) может создавать высококлассные модели. Более мелкие или менее известные игроки могут появиться из ниоткуда и выпустить на рынок модель уровня o1.\n",
      "\n",
      "Соперничество между Китаем и США в области развития ИИ также становится все более интригующим. Deepseek показала, что возможно при более открытом и прозрачном подходе. Является ли эта прозрачность и дешевизна модели стратегическим бизнес-ходом, философской позицией в отношении ИИ как общественного блага или политическим преимуществом, пока неясно.\n",
      "\n",
      "Вы можете бесплатно пообщаться с DeepSeek R-1 на chat.deepseek.com или изучить документацию по API здесь. Бенчмарки, технические подробности и файлы для загрузки моделей можно найти в репозитории на GitHub.\n",
      "\n",
      "Друзья, буду рад, если вы подпишетесь на мой телеграм-канал про нейросети, чтобы не пропускать анонсы статей, и про генерацию изображений - я стараюсь делиться только полезной информацией.\n",
      "\n",
      "Неужели DeepSeek скопировала OpenAI? Неужели DeepSeek скопировала OpenAI? В последние несколько дней вокруг китайского ИИ-стартапа DeepSe... habr.com\n",
      "\n",
      "DeepSeek выпустила собственный ИИ-генератор изображений Janus-Pro. Лучше ли он, чем Dall-e 3? DeepSeek выпустила собственный ИИ-генератор изображений Janus-Pro Модель R-1 от DeepSeek в последние... habr.com\n",
      "\n",
      "\n",
      "Акции американских и европейских производителей микрочипов на торгах понедельника потеряли в цене 12–19%. А все из-за китайской компании DeepSeek, которая выпустила новую модель искусственного интеллекта. Компания заверяет, что ее чат-бот не проигрывает западным аналогам по ключевым характеристикам, а главное, куда дешевле в разработке и работает на менее продвинутых микрочипах. А это может существенно изменить расстановку сил в технологическом секторе.\n",
      "\n",
      "Выйти из полноэкранного режима Развернуть на весь экран Фото: CFOTO / Future Publishing / Getty Images Фото: CFOTO / Future Publishing / Getty Images\n",
      "\n",
      "Мировые фондовые рынки в понедельник оказались в состоянии паники. Фьючерсы NASDAQ 100 в ходе внебиржевых торгов подешевели на 5,2%, а европейский технологический индекс Stoxx 600 в рамках торговой сессии просел на 4,5%. Таким образом, суммарные потери рыночной капитализации участников этих индексов в моменте превысили $1 трлн.\n",
      "\n",
      "Причиной столь масштабного падения стало заявление малоизвестной китайской компании DeepSeek о том, что она может создавать языковые модели не хуже западных конкурентов вроде OpenAI или Meta (признана экстремистской и запрещена в РФ), но при этом в несколько раз дешевле.\n",
      "\n",
      "DeepSeek недавно выпустила сразу две языковые модели — бесплатную V3 и платную R1. А на минувших выходных «умный помощник» китайского разработчика добрался до вершины App Store, обойдя по количеству скачиваний своего главного конкурента — ChatGPT от OpenAI.\n",
      "\n",
      "Стремительное покорение магазинов приложений китайской компанией объясняется просто: цены на использование DeepSeek R1 в среднем на 96% дешевле, чем на передовую GPT-4o OpenAI.\n",
      "\n",
      "В заметном плюсе оказался и сам разработчик: обучение языковой модели обошлось китайской компании всего в $5,5 млн. Для сравнения: по данным аналитической фирмы Epoch AI, OpenAI потратила на обучение ChatGPT от $41 млн до $78 млн.\n",
      "\n",
      "Но самое главное, DeepSeek обучала свою модель на далеко не самых передовых графических процессорах Nvidia H800. Их американская компания разработала специально для поставок в Китай, урезав мощность так, чтобы те не подпадали под санкции США. В октябре прошлого года администрация Джо Байдена запретила и их поставку в Китай, но на тот момент у DeepSeek были уже тысячи этих процессоров.\n",
      "\n",
      "Как утверждает сама китайская компания, ее модель R1 не уступает GPT-4o по ключевым показателям. А значит, на рынке моделей искусственного интеллекта появился конкурент, который не требует таких затрат на оборудование, как считалось до сих пор. И это очень плохие новости для всех тех, кто в последние годы фиксировал значительный рост прибыли от возникшего спроса на передовые микрочипы.\n",
      "\n",
      "Котировки акций американских производителей микрочипов в понедельник полетели вниз: Nvidia на премаркете теряла более 11,5% рыночной стоимости, Broadcom — более 12%, Micron — 7,5%.\n",
      "\n",
      "На более чем 7% подешевел крупнейший в мире производитель оборудования для передовых микрочипов, нидерландская ASML. А акции японской Softbank, недавно вложившейся в проект Stargate на $500 млрд совместно с OpenAI и Oracle, по итогам торгового дня на Токийской бирже упали на 8,3%.\n",
      "\n",
      "Впрочем, от DeepSeek досталось не только участникам индустрии полупроводников, но и криптовалютному рынку. Так, курс биткойна на выходных упал ниже психологической отметки в $100 тыс. А все потому, что многие инвесторы тоже поверили в силу китайского искусственного интеллекта и спросили его о будущем криптовалют. Прогнозы чат-бота оказались неутешительными: неминуемое ужесточение денежно-кредитной политики и регулирования цифровых валют. А значит, надо продавать.\n",
      "\n",
      "На 21:00 МСК на NASDAQ падение котировок Nvidia достигло 16,7%, Broadcom — 18,5%, Micron — 12,7%, индекс NASDAQ Composite потерял 3,4%, опустившись ниже 193000 пунктов.\n",
      "\n",
      "Кирилл Сарханянц\n",
      "\n",
      "\n",
      "Китай бросил вызов Дональду Трампу в сфере искусственного интеллекта. Компания DeepSeek выложила свою модель ИИ в день инаугурации президента США и обрушила акции западных техногигантов. Несмотря на отсутствие новейших американских чипов, разработка из КНР может даже превосходить конкурента от OpenAI, считают на рынке. Насколько справедливы такие оценки? И как появление модели влияет на индустрию? Расскажет Даниил Бабкин.\n",
      "\n",
      "Выйти из полноэкранного режима Развернуть на весь экран Фото: CFOTO / Future Publishing / Getty Images Фото: CFOTO / Future Publishing / Getty Images\n",
      "\n",
      "В DeepSeek утверждают, что их разработка такая же мощная, как и продукт от лидера рынка OpenAI. Но американская компания потратила на обучение своей модели около $80 млн. А китайская разработка стоила всего $5,5 млн. После публикаций западных СМИ инвесторы задались вопросом: не завышены ли оценки стартапов, связанных с искусственным интеллектом? Бумаги технологических компаний и Европы, и США заметно подешевели. Удивление рынка вызвано тем, что из-за санкций Китай не может получать новейшие чипы.\n",
      "\n",
      "Поэтому в стране создали технологию, менее требовательную к оборудованию, отмечает член комиссии по стандартизации искусственного интеллекта Французской ассоциации по стандартизации Александр Тюльканов: «У нас есть новая интересная модель. По некоторым бенчмаркам утверждается, что она сопоставима с одной из последних, опубликованных OpenAI. Систему на основе модели можно развернуть практически у себя дома. Сейчас проходят эксперименты вплоть до запуска их на компьютерах, которые обычно считались неподходящими для этого. Именно работа обходится гораздо дешевле, по некоторым оценкам, в 21 раз, чем почти аналогичная от OpenAI».\n",
      "\n",
      "Воспринимают DeepSeek как «черного лебедя», который изменит рынок, но пока только в техноблогах, а в уважаемых изданиях от громких оценок воздерживаются. И тем не менее подтверждают, что китайская модель особенно хороша в программировании и математических расчетах. Тревога на Уолл-стрит обоснована, считает основатель и генеральный директор компании Sistemma Сергей Зубарев: «Китай абсолютно не регулирует развитие искусственного интеллекта, он, наоборот, это поощряет. В КНР больше всего открытий и научных публикаций на данную тему, чем во всем остальном мире. Отличная китайская математическая школа привела к тому, что они компенсируют недостаток финансов, видеокарт за счет оптимизации алгоритмов.\n",
      "\n",
      "Абсолютная угроза для ChatGPT как для лидера».\n",
      "\n",
      "Одна из ключевых особенностей модели — полностью открытый код, и возможность для разработчиков широко использовать ее для коммерческих целей. Она доступна и в России без VPN, но требует регистрации. Выход DeepSeek на рынок пойдет всем на пользу, считает руководитель образовательной программы «Искусственный интеллект» МИФИ Роман Душкин: «В этом случае конкуренция должна принести благо.\n",
      "\n",
      "Давление, которое оказывают китайские компании, приведет к тому, что модели станут более качественные, с одной стороны, и, скорее всего, более открытые, с другой. Сейчас выбран курс на ИИ-национализацию — все замкнуть в национальных границах, никому ничего не давать, несмотря на то, что это технология, которая, является подрывной для мировой экономики в целом.\n",
      "\n",
      "И именно мировому сообществу нужно в тесном диалоге друг с другом вырабатывать новые правила игры».\n",
      "\n",
      "На прошлой неделе Дональд Трамп объявил о создании проекта Stargate. OpenAI и Oracle и SoftBank вложат в него сразу $100 млрд. Президент США обещал, что проект станет крупнейшим в сфере искусственного интеллекта в истории. А 27 января китайский Bank of China сообщил, что инвестирует в искусственный интеллект 1 трлн юаней, или $138 млрд.\n",
      "\n",
      "С нами все ясно — Telegram-канал \"Ъ FM\".\n",
      "\n",
      "Даниил Бабкин, Илья Сизов\n",
      "\n",
      "\n",
      "深度求索, запомните эти слова.\n",
      "\n",
      "Пока скептики в области ИИ продолжают размышлять о том, что ИИ никогда не обретёт сознание и останется лишь имитатором интернета, небольшая китайская компания совершила прорыв. Они создали думающую модель всего за $6 млн (меньше, чем зарплата некоторых инженеров ИИ в Кремниевой долине, и 2% от стоимости ближайшего конкурента), которая не уступает OpenAI o1, являясь при этом открытой и доступной по цене - $2,50 за обработку миллиона токенов в самой дорогой модели ChatGPT против $0,14 у DeepSeek. Доступ к API не требует VPN и другого геморроя и обходится в разы дешевле, мы уже переключили часть проектов с других моделей на DeepSeek.\n",
      "\n",
      "Попробуйте сами — эта модель настолько хороша, что вероятно, превзойдет в способности размышлять многих читателей этой статьи.\n",
      "\n",
      "Всё это произошло, пока американские гиганты, вроде Google, ковыряли в носу (как и компании в Европе и у нас кстати тоже) тратили миллиарды, что вызвало панику в отрасли. DeepSeek добилась успеха, оптимизируя железо и позволив модели обучать себя самостоятельно.\n",
      "\n",
      "Некоторые выражают опасения о приватности и реальной стоимости разработки, считая, что проект косвенно финансируется КПК. Пока доказательств нет, но я подозреваю, что в этом есть доля правды (читайте далее).\n",
      "\n",
      "Этот скачок вызвал настоящую истерику в Кремниевой долине. Корпорации экстренно собирают совещания, ведь Китай неожиданно вышел в лидеры, обойдя санкции США, и гонка ИИ теперь в самом разгаре. Как я говорил ранее, США не могут позволить Китаю доминировать в ИИ. Учитывая, что нынешнее правительство США состоит из технократов вроде Маска, стоит ждать ответа, аналогичного запуску Спутника в СССР — триллионы долларов будут вложены в эту гонку.\n",
      "\n",
      "Что известно о DeepSeek?\n",
      "\n",
      "Акт 1. Ботаники, случайно накопившие гору GPU\n",
      "\n",
      "История начинается не в гараже Кремниевой долины, а в мире финансов — где математики печатают деньги, пока мы спорим о биткоинах. High-Flyer Quant, китайский хедж-фонд, основанный в 2015 году Ляном Вэньфэном. В 2021-м, до санкций США, он закупил GPU «на всякий случай». Их использовали для анализа рынка, но большую часть времени мощности простаивали, и Лян решил занять их чем-то интересным.\n",
      "\n",
      "К 2021-му у High-Flyer было 10 000 GPU — достаточно для съемок «Трансформеров» в реальной жизни. Говорят, глава NVIDIA прислал Ляну открытку с надписью: «Спасибо за яхту». Но вместо прогнозирования мемных акций Лян решил построить AGI.\n",
      "\n",
      "В 2023-м High-Flyer выделил ИИ-направление в DeepSeek. Акционеры недоумевали («Простите, мы вместо зарабатывания денег нам на новую яхту занимаемся чем???111»), но Лян настаивал: «Представьте ChatGPT, но дешевле… созданный теми, кто не уходит из офиса даже чтобы поспать».\n",
      "\n",
      "Ранние дни DeepSeek — мастер-класс по хаосу. Офисная культура - сотрудники кодили по 18 часов в сутки, подпитываясь только bubble-tea и экзистенциальным страхом.\n",
      "\n",
      "Найм сотрудников - «У вас есть PhD? Отлично. А Вы знаете, что такое work-life balance? Печально.\n",
      "\n",
      "Финансирование - полностью за счет прибыли High-Flyer. Ничто так не кричит «стратегия», как ставка на AGI на деньги хедж-фонда.\n",
      "\n",
      "Акт 2. Инженерная магия: как обойти санкции\n",
      "\n",
      "Пока США спорили об этике AI, DeepSeek избрала подход «Подержите мой смузи»:\n",
      "\n",
      "Прорыв в архитектуре: Multi-head Latent Attentio n (MLA) — метод, сокративший стоимость обучения на 90% за счет игнорирования 95% данных. Гениально или лениво?\n",
      "\n",
      "Модели MoE : DeepSeek-V2 с 236 млрд параметров обошёлся дешевле, чем сезон Stranger Things. Инженеры отметили это покупкой новых GPU (дядя Илон одобряет такой уровень упоротости).\n",
      "\n",
      "Обучение с подкреплением: Модели серии R1 учили математику методом проб и ошибок, как дети с PhD. Результат? Уровень GPT-4, но на 95% дешевле.\n",
      "\n",
      "В мае 2024-го DeepSeek шокировала ценами на API: 2 юаня за миллион токенов. Перевод: «Мы разорим ваш стартап за стоимость упаковки соевого молока».\n",
      "\n",
      "Последствия:\n",
      "\n",
      "Alibaba и Tencent режут цены быстрее, чем уличные торговцы убегают от проверок.\n",
      "\n",
      "В Кремниевой долине — паника. OpenAI уже объявил что o3 mini будет включен в бесплатную подписку и тихо обновляет прайсы, бормоча «Это нечестно».\n",
      "\n",
      "Цукерберг выложил ответ на сайте своей экстремистской организации, от том что компания планирует сделать, учитывая что DeepSeek превосходит LLama в тестах.\n",
      "\n",
      "А что делает DeepSeek? Выпускает все в open-source. И это пока в Долине размышляют: «Лицензия MIT? А мы вообще можем это запатентовать?!». Ян Лекун назвал это «the most elegant middle finger to proprietary AI».\n",
      "\n",
      "Конечный результат немного шокирует:\n",
      "\n",
      "Акт 3. Споры\n",
      "\n",
      "В интернете сразу появилась целая куча идиотов, которые сразу начали ныть, что DeepSeek использовать не стоит, потому что он предвзят. «Посмотрите — он не отвечает, кому принадлежит Тайвань» или «Посмотрите, он не отвечает на вопросы про Си Цзиньпина». Подобная слепота просто пугает. Их должно волновать только то, что именно китайцы разработали эту модель, иначе скоро всем нам самим придётся говорить, что Тайвань принадлежит Китаю и возможно даже на китайском (хотя мне лично это совершенно по барабану).\n",
      "\n",
      "Также вызывает большое сомнение красивая история изложенная выше, что небольшой фонд с небольшими средствами смог сделать такой прорыв. Далеко не исключено что правительство Китая косвенно помогло компании. Но, в конечном итоге, это не важно, важен сам результат.\n",
      "\n",
      "Эпилог: AGI or bust\n",
      "\n",
      "Создаст ли DeepSeek AGI? Бог его знает, но они уже доказали: в гонке ИИ побеждает не тот, кто тратит больше, а тот, кто целеустремленнее и упоротее — как Маск уже неоднократно доказал в разных отраслях, типа ракетостроения и электромобилей. То, что модель полностью открыта, несомненно даст значительное ускорение всей отрасли. У нас уже кончаются бенчмарки для AI, чтобы доказать что он не способен думать. Недавно даже ввели финальный бенчмарк.\n",
      "\n",
      "Я думаю, что отрицание реальности уже более не является разумным курсом для нас.\n",
      "\n",
      "Я из Рафт. Мой телеграм-канал.\n",
      "\n",
      "Задавайте свои ответы в комментариях.\n",
      "\n",
      "祝大家一切顺利！\n",
      "\n",
      "\n",
      "Сегодняшняя новость настолько значима, что я не могу обойти её стороной. Расскажу вам несколько интересных фактов, связанных с китайской нейронной сетью DeepSeek, которая буквально встряхнула мировой технологический ландшафт.\n",
      "\n",
      "Почему модель от DeepSeek R1 за сутки стала лидером?\n",
      "\n",
      "Успеху DeepSeek во многом способствовала новая архитектура Multi-head Latent Attention (MLA), которая позволила сократить стоимость обучения на 90%, игнорируя 95% ненужных данных. Вопрос остаётся открытым: это гениальное упрощение или просто экономия на алгоритмах? Как бы то ни было, результат поражает, что DeepSeek обогнал ChatGPT по всем основным бенчмаркам. Модели серии R1 обучались математике методом проб и ошибок, как аспиранты, и в итоге достигли уровня GPT-4, но при этом оказались на 95% дешевле.\n",
      "\n",
      "Сравнение R1 c ChatGPT\n",
      "\n",
      "В бесплатной версии ChatGPT существует ограничение на количество запросов, которые вы можете отправить. Это число варьируется в зависимости от нескольких факторов, главным из которых является длина и сложность ответа, который вы хотите получить.\n",
      "\n",
      "В среднем, можно рассчитывать на 20–50 запросов в час. Если вы задаете короткие и простые вопросы, такие как «Какая погода сегодня?», лимит будет ближе к 50. Однако, если вам требуется развернутый и детализированный ответ на сложный запрос, например, «Напишите эссе о влиянии искусственного интеллекта на общество», количество доступных запросов может сократиться до 20 или даже меньше.\n",
      "\n",
      "На платформе chat.deepseek.com я тестировал работу чат-бота DeepSeek и за всё время использования не столкнулся с оплатой или ограничениями. Возможно, у DeepSeek есть скрытые лимиты, но я их не достиг, даже при активной нагрузке.\n",
      "\n",
      "Кроме того, производительность DeepSeek приятно удивила. В отличие от многих других ИИ-чатов, которые могут замедляться в часы пик или при высокой интенсивности запросов, модель R1 работала быстро и стабильно с самого начала и до конца.\n",
      "\n",
      "Новая модель DeepSeek R1 не только догнала, но и превзошла OpenAI по ключевым показателям, при этом оставаясь открытой и невероятно доступной по цене. Стоимость обработки миллиона токенов у DeepSeek составляет всего 0,14$ в то время, как самая дорогая модель ChatGPT за аналогичный объем - 2,5$. DeepSeek имеет окно в 128 тысяч токенов, а Chat GPT всего лишь 32 тысячи токенов, и то зависит от модели. Один токен эквивалентно равен 4 символам. Вот и делайте умножения сколько глав или какого объема текста можно написать в режиме одного окна.\n",
      "\n",
      "ПРО ЖЕЛЕЗО и ЗАТРАТЫ\n",
      "\n",
      "DeepSeek использует всего лишь 10 тысяч видеокарт — мощности, для сравнения модель Llama от Цукерберга использует 300 тысяч видеокарт. Для обучения были использованы старые чипы Nvidia, что также помогло значительно сократить затраты (пока СМИ не уточнили, какие именно чипы). При этом стоимость создания DeepSeek составила лишь 2% от инвестиций в OpenAI — скромные $12 миллионов, что несопоставимо меньше по сравнению с $500 миллионами, затраченными на GPT-5 от OpenAI. Интересный факт годовая зарплата некоторых инженеров ИИ в Кремниевой долине такая же, как стоимость разработки. По моим подсчетам, 10 тысяч видеокарт это приблизительно 100 серверных стоек, поэтому коммерческие дата-центры тоже будут набирать обороты в след за ИИ.\n",
      "\n",
      "Что происходит на рынке сейчас?\n",
      "\n",
      "Китайский чат-бот DeepSeek мгновенно взлетел на первое место в топе AppStore в шести странах, включая США, обогнав даже ChatGPT на его родной территории.\n",
      "\n",
      "Акции технологических гигантов, таких как Nvidia, упали на 17%, Microsoft — почти на 5%. Американский рынок в совокупности потерял 1 триллион долларов за сутки.\n",
      "\n",
      "DeepSeek стала первой моделью с открытым исходным кодом, доступной каждому разработчику без необходимости использования VPN с производительностью наравне с моделью o1 от OpenAI.\n",
      "\n",
      "Китай инвестирует ¥1 трлн ($137 млрд) в развитие искусственного интеллекта в ближайшие пять лет. Это решение стало прямым ответом на амбициозный проект Stargate, бюджет которого оценивается в $500 млрд. Однако, в отличие от OpenAI, которая будет получать финансирование постепенно, китайские модели ИИ могут получить значительное преимущество благодаря единовременному вливанию средств.\n",
      "\n",
      "Этот технологический скачок вызвал настоящую истерику среди американских корпораций. Экстренные совещания, пересмотр стратегий и миллиардные инвестиции — всё это стало реакцией на неожиданное лидерство Китая. Китай не только обошёл санкции США, но и вышел вперёд в гонке искусственного интеллекта.\n",
      "\n",
      "Чем феномен DeepSeek полезен для маленьких компаний?\n",
      "\n",
      "Представьте, что вы разработчик или основатель стартапа, который хочет внедрить большую языковую модель в свой продукт. На сегодняшний день модели OpenAI серии o1 или GPT занимают лидирующие позиции, но их использование через API может стать серьёзным ударом по бюджету, особенно для малых и средних компаний. В этом плане DeepSeek предлагает альтернативу с гораздо более доступными тарифами, которые могут значительно сократить расходы.\n",
      "\n",
      "$0,14 за миллион входных токенов (если запрос попадает в кэш)\n",
      "\n",
      "$0,55 за миллион входных токенов (если запрос проходит без кэширования)\n",
      "\n",
      "$2,19 за миллион выходных токенов\n",
      "\n",
      "Теперь разберём, что такое входные и выходные токены. Входные токены представляют собой части текста, которые вы отправляете в модель, будь то запрос пользователя, строка кода или любой другой текст для обработки. Выходные токены — это ответ модели, то есть тот текст, который вы получаете на ваш запрос. Для понимания: 1 токен примерно соответствует 4 символам текста, включая пробелы. Например, слово «Привет!» состоит из 2 токенов, а предложение из 50 слов будет занимать около 100–120 токенов.\n",
      "\n",
      "Стоимость токенов напрямую влияет на то, насколько рентабельным будет использование модели, особенно при массовых запросах. Например, небольшие компании, которые обрабатывают десятки тысяч запросов в день, сталкиваются с необходимостью платить высокие суммы при использовании дорогих API. В случае с DeepSeek это может означать экономию в 10–30 раз, что делает внедрение ИИ доступным даже для стартапов с ограниченными ресурсами.\n",
      "\n",
      "Более того, низкая стоимость входных токенов при кэшировании ($0,14) является значительным преимуществом для задач, где запросы часто повторяются. Это может быть крайне полезно для чат-ботов, онлайн-помощников или аналитических систем, работающих с повторяющимися данными. Компании могут не только снизить затраты, но и инвестировать сэкономленные средства в другие аспекты своего бизнеса — будь то маркетинг, разработка или улучшение пользовательского опыта.\n",
      "\n",
      "DeepSeek открывает двери для малых компаний и стартапов, позволяя использовать передовые технологии без колоссальных финансовых вложений, и это делает её отличным выбором для бизнеса любого масштаба.\n",
      "\n",
      "Цены для сравнения, взял тут\n",
      "\n",
      "Вывод\n",
      "\n",
      "Соперничество между Китаем и США в сфере развития искусственного интеллекта продолжает набирать обороты, становясь всё более захватывающим и напряжённым. DeepSeek наглядно продемонстрировала, каких результатов можно достичь, придерживаясь более открытого и прозрачного подхода. Однако остаётся неясным, что именно стоит за этой прозрачностью и доступностью модели: стратегический бизнес-ход, философская убеждённость в том, что ИИ должен быть общественным благом, или же стремление получить политическое преимущество на мировой арене.\n",
      "\n",
      "Этот вопрос остаётся открытым, но одно можно сказать с уверенностью — что мир технологий меняется с невероятной скоростью. И те, кто сегодня кажется лидером, завтра могут оказаться в роли догоняющих.\n",
      "\n",
      "Вот ссылка на чат-бот от DeepSeek.\n",
      "\n",
      "Приглашаю Вас в свой telegram‑канал «охота за технологиями», там я пишу о технологиях, которые завоевывают признание миллионов людей, а также делюсь срочными новостями и провожу короткое расследование и ревью по ним. Я называю свой канал «Пространство для стратегов и новаторов, для тех, кто меняет правила игры и готов пойти на риск ради будущего, разрушив старые стереотипы. Буду ждать Вас там!»\n",
      "\n",
      "\n",
      "Поделиться\n",
      "\n",
      "Китайская модель ИИ DeepSeek стала лидером по скачиванию в США\n",
      "\n",
      "Продукт от стартапа, основанного всего два года назад, оказался производительнее почти всех конкурирующих аналогов от американских компаний. DeepSeek требует куда менее мощного оборудования, и его обучение обходится почти в 100 раз дешевле\n",
      "\n",
      "Фото: IMAGO/CFOTO/TASS\n",
      "\n",
      "Обновлено в 14:41\n",
      "\n",
      "Новая китайская модель ИИ DeepSeek стала самым скачиваемым приложением в США, обрушила NASDAQ, отправила акции Nvidia в пике. Как пишет Bloomberg, китайский DeepSeek ставит под сомнение доминирующую роль США в области искусственного интеллекта.\n",
      "\n",
      "Последняя версия нейросети китайского стартапа DeepSeek работает на чипах с ограниченной производительностью. Это значит, что она экономически более выгодна, потому что для нее — в отличие от других моделей — не требуется крупных капиталовложений. О китайской новинке говорит генеральный директор IT-компании «А-Я эксперт», старший преподаватель кафедры «Кибернетика» НИЯУ МИФИ Роман Душкин:\n",
      "\n",
      "Роман Душкин главный архитектор систем искусственного интеллекта исследовательского центра ИИ по направлению «Транспорт и логистика» НИЯУ МИФИ\n",
      "\n",
      "На фоне ажиотажа вокруг китайской новинки акции Nvidia, взлетевшие благодаря буму в сфере ИИ, упали более чем на 3% и на премаркете — на 10%. Между тем биржевые боты DeepSeek по запросу «Бизнес FM» придумать анекдот о ситуации шутят: «Раньше майнили крипту, теперь майним панику!» О том, что меняет появление новой модели, и ее перспективах говорит основатель и директор по развитию «Промобот» Олег Кивокурцев:\n",
      "\n",
      "Олег Кивокурцев основатель и директор по развитию «Промобот»\n",
      "\n",
      "Конкурирующая с американскими техногигантами китайская DeepSeek была основана менее двух лет назад. Разработка стоила менее 10 млн долларов, а в компании трудятся всего 200 человек. Для сравнения, OpenAI существует уже десять лет, имеет более 4 тысяч сотрудников и привлеченный капитал больше 6,5 млрд долларов.\n",
      "\n",
      "\n",
      "Эксперимент DeepSeek-R1-Zero показал нечто замечательное: используя чистое обучение с подкреплением с тщательно продуманными функциями вознаграждения, им удалось заставить модели развивать сложные способности рассуждения полностью автономно. Речь шла не только о решении проблем — модель органически научилась генерировать длинные цепочки мыслей, самостоятельно проверять свою работу и выделять больше вычислительного времени для более сложных задач.\n",
      "\n",
      "Техническим прорывом здесь стал их новый подход к моделированию вознаграждения. Вместо того чтобы использовать сложные нейронные модели вознаграждения, которые могут привести к «взлому вознаграждения» (когда модель находит фиктивные способы увеличить свои вознаграждения, которые на самом деле не приводят к лучшей производительности модели в реальном мире), они разработали умную систему на основе правил, которая сочетает вознаграждения за точность (проверку окончательных ответов) с вознаграждениями за формат (поощрение структурированного мышления). Этот более простой подход оказался более надежным и масштабируемым, чем модели вознаграждения на основе процесса, которые пробовали другие.\n",
      "\n",
      "Я начинаю по-настоящему верить в SaaS / AI-агентов. Это будет мегатренд, о котором сейчас никто и не подозревает. Я уверен, что на этом пути будет много взлетов и падений.\n",
      "\n",
      "Deepseek теперь №1 в AppStore, обогнав ChatGPT — не нужны суперкомпьютеры NVIDIA или 100 млн долларов. Настоящее сокровище ИИ — это не пользовательский интерфейс или модель — они стали товарами. Истинная ценность заключается в данных и метаданных, кислороде, питающем потенциал ИИ. Будущие богатства — в наших данных.\n",
      "\n",
      "Не нужно платить 200$ за использование Operator. Вы можете создать агента, использующего веб-браузер, не написав ни строчки кода. Объедините DeepSeek R1 и Browser Use (бесплатный и с открытым исходным кодом), и все готово. А приложение RAG поможет с обменом данными с вашими PDF-файлами с использованием модели DeepSeek R1, работающей локально на вашем компьютере.\n",
      "\n",
      "Я установил локальную версию DeepSeek R1 на свой старенький мак всего за 3-4 минуты и меня удивила производительность. Теперь у меня есть свой мощный и шустрый ИИ-помощник, который всегда со мной, даже если у меня нет интернета.\n",
      "\n",
      "Если кто-то хочет попробовать работу этого помощника, ставьте плюс в комментариях, выложу 6 скриншотов как быстро установить DeepSeek R1. Попробуйте с простой версии, но если у вас шустрая и мощная машина, можете выбрать из списка версию помощнее.\n",
      "\n",
      "\n",
      "Понедельник, 27 января 2025 года, войдёт в историю как один из худших дней для технологических компаний со всего мира — акции большинства из них упали на фоне успеха китайского ИИ-стартапа DeepSeek. Хуже всех пришлось компании Nvidia — её капитализация рухнула примерно на $600 млрд, что является крупнейшим обвалом в истории фондового рынка США. И, возможно, это ещё не конец.\n",
      "\n",
      "На момент подготовки данного материала акции Nvidia показывали суточное падение в 17,8 % — для данной компании это самый серьёзный спад с марта 2020 года. Рыночная стоимость крупнейшего производителя ИИ-чипов в мире сократилась на 600 млрд долларов до отметки в 2,89 трлн, что является рекордным падением в истории. Прежний рекорд в 279 млрд также принадлежал Nvidia и произошёл в сентябре 2024 года. Ещё вчера Nvidia была самой дорогой компанией в мире, а уже сегодня скатилась на третье место после Apple и Microsoft, и рискует опуститься ещё ниже.\n",
      "\n",
      "Примеру ценных бумаг Nvidia последовали акции многих других компаний технологического сектора, так или иначе связанных с ИИ. Акции Broadcom потеряли 17,3 %, AMD — 8 %, Microsoft — 3 %, Palantir — 7 %. Пожалуй, OpenAI повезло, что она не торгуется на бирже, поскольку её акции скорее всего тоже были бы в лидерах падения. Индекс Nasdaq Composite потерял 3,5 %, а индекс S&P 500 упал на 1,8 %. Добавим, что пострадали и компании, не связанные с ИИ напрямую: например, поставщики электроэнергии Constellation Energy и Vistra потеряли за день 21 и 29 % своей стоимости соответственно. Всего фондовый рынок США за день потерял более 1 трлн долларов капитализации.\n",
      "\n",
      "Почему же инвесторы устроили распродажу акций и обвалили рынки? Всё дело в китайском стартапе DeepSeek, который нашёл способ обучать продвинутые ИИ-модели на малом количестве ускорителей вычислений. Например, при обучении одной из своих моделей на внушительных 671 млрд параметров DeepSeek использовала всего 2048 ИИ-ускорителей Nvidia H800 и потратила $5,6 млн. Это мизерная часть расходов OpenAI и Google на обучение моделей сопоставимого размера.\n",
      "\n",
      "Кроме того, на прошлой неделе DeepSeek выпустила «рассуждающую» модель ИИ R1, которая превзошла мыслящую OpenAI o1 в важных тестах. Более того, компания опубликовала инструкции, как с минимальными затратами построить большую языковую модель, способную самостоятельно обучаться и совершенствоваться без контроля со стороны человека. Добавим, что многие свои разработки DeepSeek распространяет совершенно бесплатно. Да и платный доступ к наиболее продвинутым её системам оказывается намного дешевле, чем у конкурентов — например, R1 доступна через API компании по цене, которая на 90–95 % ниже, чем у OpenAI o1.\n",
      "\n",
      "В итоге инвесторы поняли, что нейросети можно обучать не только по схеме «купи и установи как можно больше ускорителей вычислений», но и куда более эффективно на меньшем числе GPU. Это грозит резким падением спроса на продукцию Nvidia, выручка которой более чем на 80 % зависит как раз от ускорителей вычислений.\n",
      "\n",
      "Ряд аналитиков предрекает Nvidia мрачное будущее и не рекомендуют пока покупать акции компании, хотя цены на них стали заманчивыми. Другие же наоборот уверены, что компания сможет быстро восстановиться, а нынешний спад как раз следует использовать для покупки акций.\n",
      "\n",
      "Что интересно, сама Nvidia похвалила разработки DeepSeek. Компания отметила, что новая модель DeepSeek R1, является «отличным достижением в области ИИ», которое не нарушает экспортные ограничения США. Заявление также отвергает подозрения некоторых аналитиков и экспертов в том, что китайский стартап не мог совершить тот прорыв, о котором он заявлял.\n",
      "\n",
      "Вместе с тем в Nvidia отметили, что её ускорители нужны не только для обучения ИИ-моделей, но и для инференса — запуска уже обученных систем. Причём для этого нужно очень много GPU, особенно при большом числе пользователей. «Для инференса требуется значительное количество графических процессоров Nvidia и высокопроизводительные сети», — заявили в компании.\n",
      "\n",
      "\n",
      "По информации иностранных СМИ, Microsoft и OpenAI расследуют, обучался ли DeepSeek на украденных данных американских компаний в сфере ИИ. В Microsoft проверяют ситуацию в отношении «связанной с DeepSeek компании», которая могла без разрешения получить большой объём данных посредством открытого API американской ИИ‑системы OpenAI.\n",
      "\n",
      "Исследователи безопасности из Microsoft осенью прошлого года зафиксировали, что лица, которые, по их мнению, могут быть связаны с DeepSeek, извлекали большие объёмы данных с помощью интерфейса прикладного программирования OpenAI API.\n",
      "\n",
      "Microsoft, технологический партнёр OpenAI и её крупнейший инвестор, уведомила OpenAI об этой деятельности. Подобные действия могли нарушать условия обслуживания OpenAI или может указывать на то, что определённая группа действовала для снятия ограничений OpenAI на объём данных, которые они могут получить.\n",
      "\n",
      "Представители администрации США допустили, что китайская ИИ‑компания могла обучать свою модель DeepSeek R1 на основе моделей OpenAI посредством дистилляции — метода передачи знаний из крупной сложной модели (часто называемой моделью преподавателя) на меньшую, простую модель (модель учащегося). Этот процесс помогает модели учащегося достичь аналогичной производительности большей. Глава отдела по искусственному интеллекту и криптовалютам в Белом доме Дэвид Сакс не раскрыл факты в обосновании этой гипотезы. Сакс, не назвавший источник этого «доказательства», предположил, что DeepSeek использовал ответы моделей OpenAI для обучения своих собственных. «Я не думаю, что OpenAI очень довольна этим», — пояснил Сакс.\n",
      "\n",
      "Согласно политике использования OpenAI, запрещено «копировать» какие‑либо её услуги или «использовать выходные данные для разработки конкурирующих моделей». Эксперты пояснили, что дистилляция — это нормальная практика, но «проблема в том, когда её задействуют, чтобы создать свою собственную модель для своих собственных целей».\n",
      "\n",
      "В OpenAI не ответили об инциденте с DeepSeek. «Мы знаем, что компании из Китая — и другие — постоянно пытаются перенять модели ведущих американских компаний в области ИИ. Как ведущий разработчик ИИ, мы принимаем контрмеры для защиты нашей интеллектуальной собственности, включая тщательный процесс включения передовых возможностей в выпускаемые модели, и считаем, что по мере продвижения вперёд критически важно тесно сотрудничать с правительством США, чтобы наилучшим образом защитить самые эффективные модели от попыток противников и конкурентов завладеть американскими технологиями», — заявил представитель OpenAI.\n",
      "\n",
      "Затраты DeepSeek на свою нейросеть могли оказаться сильно больше $5 млн, про которые ранее рассказали в самой компании. Financial Times приводит оценку аналитика SemiAnalysis, который считает, что DeepSeek могла потратить на аппаратную часть проекта больше $500 млн. Те самые $5 млн могло стоить финальное обучение. Но до этого ещё были испытания, эксперименты и выпуск предыдущих версий.\n",
      "\n",
      "Генеральный директор Scale AI Александр Ванг считает, что последняя модель ИИ DeepSeek является «потрясающей». Он также описал текущую гонку между США и Китаем как «войну ИИ».\n",
      "\n",
      "Ранее СМИ сообщили, что Совет национальной безопасности США изучает потенциальные последствия для безопасности страны из‑за активного распространения наработок китайского стартапа DeepSeek. Профильные эксперты и регуляторы разных стран советуют пользователям быть осторожным с этим проектом из‑за сбора различных данных и хранения их в Китае.\n",
      "\n",
      "Согласно DeepSeek Privacy Policy и Terms of Use, ИИ‑сервис собирает, хранит и обрабатывает на своих серверах данные пользователей, включая IP‑адреса, user‑agent, шаблоны нажатий клавиш, информацию об устройствах, куки, отчёты о сбоях и журналы производительности, операционную систему, шаблоны или ритмы нажатия клавиш и язык системы. Удаление учётной записи пользователя не стирает как минимум часть этих данных.\n",
      "\n",
      "\n",
      "Поделиться\n",
      "\n",
      "DeepSeek заподозрили в краже технологий OpenAI\n",
      "\n",
      "По мнению американского разработчика, китайский стартап использовал их для обучения собственной платформы\n",
      "\n",
      "Китайский ИИ-стартап DeepSeek заподозрили в краже технологий OpenAI для обучения собственной платформы. Об этом заявил Financial Times американский разработчик чат-бота ChatGPT.\n",
      "\n",
      "По данным OpenAI, DeepSeek применила метод так называемой «дистилляции» — процесса, при котором небольшая модель обучается на выходных данных более мощной системы, снижая затраты на разработку. Дистилляция является обычной практикой в отрасли, но у OpenAI возникли опасения, что DeepSeek может прибегать к ней для создания собственной конкурирующей модели, что является нарушением условий обслуживания сервисов американской компании.\n",
      "\n",
      "OpenAI отказалась от дальнейших комментариев и не уточняет, какими именно доказательствами располагает.\n",
      "\n",
      "\n",
      "Утверждается, что в китайских лабораториях их гораздо больше.\n",
      "\n",
      "DeepSeek, ведущая китайская лаборатория искусственного интеллекта, привлекла внимание своей передовой моделью ИИ R1. По словам Александра Ванга (Alexandr Wang), основателя Scale AI, R1 использует десятки тысяч графических процессоров NVIDIA, что делает ее высококлассным конкурентом таких американских моделей, как o1 от OpenAI и Llama от Meta.\n",
      "\n",
      "Графические ускорители Hopper от NVIDIA, включая модели H100 и H200, доминируют в разработке ИИ, даже когда NVIDIA готовит новое поколение чипов Blackwell. Ванг в интервью CNBC рассказал, что R1 отлично справилась с самым сложным тестом на масштабный ИИ, сравнявшись с ведущими американскими моделями или превзойдя их.\n",
      "\n",
      "Ванг рассказал о строгой оценке ИИ \"Последний экзамен человечества\", включающей сложные вопросы из передовых исследований в области математики, физики, биологии и химии. Он отметил:\n",
      "\n",
      "\"R1 заняла первое место или сравнялась с лучшими американскими платформами. Это знаменует собой революционный сдвиг в глобальной конкуренции в области ИИ\".\n",
      "\n",
      "Ванг подчеркнул геополитические ставки, отметив, что DeepSeek символически выпустила R1 под Рождество, бросив вызов доминированию США.\n",
      "\n",
      "Успех DeepSeek зависит от графических процессоров NVIDIA, но американский экспортный контроль ограничивает их продажи в Китай. Администрация Байдена заблокировала поставки H100 в 2022 году, что заставило NVIDIA создать модифицированные чипы H800 и A800, которые также попали под санкции к 2023 году. Ванг предположил, что китайские лаборатории обошли эти ограничения.\n",
      "\n",
      "\n",
      "Сегодняшняя новость настолько значима, что я не могу обойти её стороной. Расскажу вам несколько интересных фактов, связанных с китайской нейронной сетью DeepSeek, которая буквально встряхнула мировой технологический ландшафт.\n",
      "\n",
      "Почему модель от DeepSeek R1 за сутки стала лидером?\n",
      "\n",
      "Успеху DeepSeek во многом способствовала новая архитектура Multi-head Latent Attention (MLA), которая позволила сократить стоимость обучения на 90%, игнорируя 95% ненужных данных. Вопрос остаётся открытым: это гениальное упрощение или просто экономия на алгоритмах? Как бы то ни было, результат поражает, что DeepSeek обогнал ChatGPT по всем основным бенчмаркам. Модели серии R1 обучались математике методом проб и ошибок, как аспиранты, и в итоге достигли уровня GPT-4, но при этом оказались на 95% дешевле.\n",
      "\n",
      "Сравнение R1 c ChatGPT\n",
      "\n",
      "В бесплатной версии ChatGPT существует ограничение на количество запросов, которые вы можете отправить. Это число варьируется в зависимости от нескольких факторов, главным из которых является длина и сложность ответа, который вы хотите получить.\n",
      "\n",
      "В среднем, можно рассчитывать на 20–50 запросов в час. Если вы задаете короткие и простые вопросы, такие как «Какая погода сегодня?», лимит будет ближе к 50. Однако, если вам требуется развернутый и детализированный ответ на сложный запрос, например, «Напишите эссе о влиянии искусственного интеллекта на общество», количество доступных запросов может сократиться до 20 или даже меньше.\n",
      "\n",
      "На платформе chat.deepseek.com я тестировал работу чат-бота DeepSeek и за всё время использования не столкнулся с оплатой или ограничениями. Возможно, у DeepSeek есть скрытые лимиты, но я их не достиг, даже при активной нагрузке.\n",
      "\n",
      "Кроме того, производительность DeepSeek приятно удивила. В отличие от многих других ИИ-чатов, которые могут замедляться в часы пик или при высокой интенсивности запросов, модель R1 работала быстро и стабильно с самого начала и до конца.\n",
      "\n",
      "Новая модель DeepSeek R1 не только догнала, но и превзошла OpenAI по ключевым показателям, при этом оставаясь открытой и невероятно доступной по цене. Стоимость обработки миллиона токенов у DeepSeek составляет всего 0,14$ в то время, как самая дорогая модель ChatGPT за аналогичный объем - 2,5$. DeepSeek имеет окно в 128 тысяч токенов, а Chat GPT всего лишь 32 тысячи токенов, и то зависит от модели. Один токен эквивалентно равен 4 символам. Вот и делайте умножения сколько глав или какого объема текста можно написать в режиме одного окна.\n",
      "\n",
      "ПРО ЖЕЛЕЗО и ЗАТРАТЫ\n",
      "\n",
      "DeepSeek использует всего лишь 10 тысяч видеокарт — мощности, для сравнения модель Llama от Цукерберга использует 300 тысяч видеокарт. Для обучения были использованы старые чипы Nvidia, что также помогло значительно сократить затраты (пока СМИ не уточнили, какие именно чипы). При этом стоимость создания DeepSeek составила лишь 2% от инвестиций в OpenAI — скромные $12 миллионов, что несопоставимо меньше по сравнению с $500 миллионами, затраченными на GPT-5 от OpenAI. Интересный факт годовая зарплата некоторых инженеров ИИ в Кремниевой долине такая же, как стоимость разработки. По моим подсчетам, 10 тысяч видеокарт это приблизительно 100 серверных стоек, поэтому коммерческие дата-центры тоже будут набирать обороты в след за ИИ.\n",
      "\n",
      "Что происходит на рынке сейчас?\n",
      "\n",
      "Китайский чат-бот DeepSeek мгновенно взлетел на первое место в топе AppStore в шести странах, включая США, обогнав даже ChatGPT на его родной территории.\n",
      "\n",
      "Акции технологических гигантов, таких как Nvidia, упали на 17%, Microsoft — почти на 5%. Американский рынок в совокупности потерял 1 триллион долларов за сутки.\n",
      "\n",
      "DeepSeek стала первой моделью с открытым исходным кодом, доступной каждому разработчику без необходимости использования VPN с производительностью наравне с моделью o1 от OpenAI.\n",
      "\n",
      "Китай инвестирует ¥1 трлн ($137 млрд) в развитие искусственного интеллекта в ближайшие пять лет. Это решение стало прямым ответом на амбициозный проект Stargate, бюджет которого оценивается в $500 млрд. Однако, в отличие от OpenAI, которая будет получать финансирование постепенно, китайские модели ИИ могут получить значительное преимущество благодаря единовременному вливанию средств.\n",
      "\n",
      "Этот технологический скачок вызвал настоящую истерику среди американских корпораций. Экстренные совещания, пересмотр стратегий и миллиардные инвестиции — всё это стало реакцией на неожиданное лидерство Китая. Китай не только обошёл санкции США, но и вышел вперёд в гонке искусственного интеллекта.\n",
      "\n",
      "Чем феномен DeepSeek полезен для маленьких компаний?\n",
      "\n",
      "Представьте, что вы разработчик или основатель стартапа, который хочет внедрить большую языковую модель в свой продукт. На сегодняшний день модели OpenAI серии o1 или GPT занимают лидирующие позиции, но их использование через API может стать серьёзным ударом по бюджету, особенно для малых и средних компаний. В этом плане DeepSeek предлагает альтернативу с гораздо более доступными тарифами, которые могут значительно сократить расходы.\n",
      "\n",
      "$0,14 за миллион входных токенов (если запрос попадает в кэш)\n",
      "\n",
      "$0,55 за миллион входных токенов (если запрос проходит без кэширования)\n",
      "\n",
      "$2,19 за миллион выходных токенов\n",
      "\n",
      "Теперь разберём, что такое входные и выходные токены. Входные токены представляют собой части текста, которые вы отправляете в модель, будь то запрос пользователя, строка кода или любой другой текст для обработки. Выходные токены — это ответ модели, то есть тот текст, который вы получаете на ваш запрос. Для понимания: 1 токен примерно соответствует 4 символам текста, включая пробелы. Например, слово «Привет!» состоит из 2 токенов, а предложение из 50 слов будет занимать около 100–120 токенов.\n",
      "\n",
      "Стоимость токенов напрямую влияет на то, насколько рентабельным будет использование модели, особенно при массовых запросах. Например, небольшие компании, которые обрабатывают десятки тысяч запросов в день, сталкиваются с необходимостью платить высокие суммы при использовании дорогих API. В случае с DeepSeek это может означать экономию в 10–30 раз, что делает внедрение ИИ доступным даже для стартапов с ограниченными ресурсами.\n",
      "\n",
      "Более того, низкая стоимость входных токенов при кэшировании ($0,14) является значительным преимуществом для задач, где запросы часто повторяются. Это может быть крайне полезно для чат-ботов, онлайн-помощников или аналитических систем, работающих с повторяющимися данными. Компании могут не только снизить затраты, но и инвестировать сэкономленные средства в другие аспекты своего бизнеса — будь то маркетинг, разработка или улучшение пользовательского опыта.\n",
      "\n",
      "DeepSeek открывает двери для малых компаний и стартапов, позволяя использовать передовые технологии без колоссальных финансовых вложений, и это делает её отличным выбором для бизнеса любого масштаба.\n",
      "\n",
      "Цены для сравнения, взял тут\n",
      "\n",
      "Вывод\n",
      "\n",
      "Соперничество между Китаем и США в сфере развития искусственного интеллекта продолжает набирать обороты, становясь всё более захватывающим и напряжённым. DeepSeek наглядно продемонстрировала, каких результатов можно достичь, придерживаясь более открытого и прозрачного подхода. Однако остаётся неясным, что именно стоит за этой прозрачностью и доступностью модели: стратегический бизнес-ход, философская убеждённость в том, что ИИ должен быть общественным благом, или же стремление получить политическое преимущество на мировой арене.\n",
      "\n",
      "Этот вопрос остаётся открытым, но одно можно сказать с уверенностью — что мир технологий меняется с невероятной скоростью. И те, кто сегодня кажется лидером, завтра могут оказаться в роли догоняющих.\n",
      "\n",
      "Вот ссылка на чат-бот от DeepSeek.\n",
      "\n",
      "Приглашаю Вас в свой telegram‑канал «охота за технологиями», там я пишу о технологиях, которые завоевывают признание миллионов людей, а также делюсь срочными новостями и провожу короткое расследование и ревью по ним. Я называю свой канал «Пространство для стратегов и новаторов, для тех, кто меняет правила игры и готов пойти на риск ради будущего, разрушив старые стереотипы. Буду ждать Вас там!»\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in (page.ref_texts).values():\n",
    "    print(text[0])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4a312-18dc-4047-acd8-da3ea801094b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c12fe-85ef-49d5-83e7-4c2d5ba7d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
