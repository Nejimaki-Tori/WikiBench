{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff89c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-03-01 10:06:51,247] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from run_bench import run_wiki_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c053f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODELS_DIR = Path(\"models\")\n",
    "BASE_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VLLM_HOST = \"127.0.0.1\"\n",
    "VLLM_PORT = 8000\n",
    "VLLM_API = f\"http://{VLLM_HOST}:{VLLM_PORT}/v1\"\n",
    "VLLM_KEY = \"local-bench-key\"\n",
    "\n",
    "\n",
    "# МОДЕЛЬ ДОЛЖНА ЛЕЖАТЬ В ПАПКЕ models/alias-модели\n",
    "MODEL_SPECS = [\n",
    "    ###### {\"alias\": \"YandexGPT-5-Lite-8B-instruct\", \"repo_id\": \"models/YandexGPT-5-Lite-8B-instruct\"},\n",
    "    \n",
    "    # --- ВСЕ ЧТО ВЫШЕ - ПОСЧИТАНО ---\n",
    "    \n",
    "    #{\"alias\": \"Qwen3-4B-Instruct\", \"repo_id\": \"Qwen/Qwen3-4B-Instruct-2507\"},\n",
    "    #{\"alias\": \"Qwen3-4B\", \"repo_id\": \"Qwen/Qwen3-4B\"},\n",
    "    #{\"alias\": \"RuadaptQwen3-4B-Instruct\", \"repo_id\": \"RefalMachine/RuadaptQwen3-4B-Instruct\"},\n",
    "    \n",
    "    ###{\"alias\": \"Qwen3-8B\", \"repo_id\": \"Qwen/Qwen3-8B\"},\n",
    "    ###{\"alias\": \"RuadaptQwen3-8B-Hybrid\", \"repo_id\": \"RefalMachine/RuadaptQwen3-8B-Hybrid\"},\n",
    "    ###{\"alias\": \"avibe\", \"repo_id\": \"AvitoTech/avibe\"},\n",
    "    ###{\"alias\": \"GigaChat3-10B-A1.8B-bf16\", \"repo_id\": \"ai-sage/GigaChat3-10B-A1.8B-bf16\"},\n",
    "    ###{\"alias\": \"T-lite-it-1.0\", \"repo_id\": \"t-tech/T-lite-it-1.0\"},\n",
    "    ###{\"alias\": \"Vikhr-Nemo-12B-Instruct-R-21-09-24\", \"repo_id\": \"Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24\"},\n",
    "    ###{\"alias\": \"RuadaptQwen2.5-7B-Lite-Beta\", \"repo_id\": \"RefalMachine/RuadaptQwen2.5-7B-Lite-Beta\"},\n",
    "    \n",
    "    #{\"alias\": \"T-pro-it-2.1\", \"repo_id\": \"t-tech/T-pro-it-2.1\"},\n",
    "    #{\"alias\": \"Qwen3-32B\", \"repo_id\": \"Qwen/Qwen3-32B\"},\n",
    "    #{\"alias\": \"RuadaptQwen3-32B-Instruct\", \"repo_id\": \"RefalMachine/RuadaptQwen3-32B-Instruct\"},\n",
    "    #{\"alias\": \"Qwen3-30B-A3B-Instruct-2507\", \"repo_id\": \"Qwen/Qwen3-30B-A3B-Instruct-2507\"},\n",
    "    #{\"alias\": \"Qwen3-14B\", \"repo_id\": \"Qwen/Qwen3-14B\"},\n",
    "    #{\"alias\": \"T-lite-it-2.1\", \"repo_id\": \"t-tech/T-lite-it-2.1\"},\n",
    "    #{\"alias\": \"T-pro-it-2.0\", \"repo_id\": \"t-tech/T-pro-it-2.0\"},\n",
    "]\n",
    "\n",
    "BENCH_CONCURRENCY = 40\n",
    "\n",
    "VLLM_GPU_MEMORY_UTIL = 0.90\n",
    "VLLM_MAX_NUM_SEQS = 20\n",
    "VLLM_MAX_MODEL_LEN = 16386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8177b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_vllm_ready(timeout_sec: int = 1200):\n",
    "    url = f\"{VLLM_API}/models\"\n",
    "    headers = {\"Authorization\": f\"Bearer {VLLM_KEY}\"}\n",
    "    deadline = time.time() + timeout_sec\n",
    "    last_error = None\n",
    "\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=5)\n",
    "            if r.ok:\n",
    "                return\n",
    "            last_error = f\"{r.status_code}: {r.text[:200]}\"\n",
    "        except Exception as e:\n",
    "            last_error = repr(e)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    raise RuntimeError(f\"vLLM не поднялся. Последняя ошибка: {last_error}\")\n",
    "\n",
    "\n",
    "def start_vllm_server(model_name: str, model_path: str) -> subprocess.Popen:\n",
    "    cmd = [\n",
    "        \"vllm\", \"serve\", model_path,\n",
    "        \"--host\", VLLM_HOST,\n",
    "        \"--port\", str(VLLM_PORT),\n",
    "        \"--api-key\", VLLM_KEY,\n",
    "        \"--served-model-name\", model_name,\n",
    "        \"--tensor-parallel-size\", \"1\",\n",
    "        \"--dtype\", \"auto\",\n",
    "        \"--gpu-memory-utilization\", str(VLLM_GPU_MEMORY_UTIL),\n",
    "        \"--max-model-len\", str(VLLM_MAX_MODEL_LEN),\n",
    "        \"--max-num-seqs\", str(VLLM_MAX_NUM_SEQS),\n",
    "        \"--swap-space\", \"16\",\n",
    "        \"--generation-config\", \"vllm\",\n",
    "        \"--disable-log-stats\",\n",
    "        \"--disable-uvicorn-access-log\",\n",
    "        \"--disable-log-requests\"\n",
    "    ]\n",
    "\n",
    "    # cmd.append(\"--trust-remote-code\")\n",
    "\n",
    "    print(f\"\\n=== START vLLM: {model_name} ===\")\n",
    "    env = os.environ.copy()\n",
    "    env[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"\n",
    "    \n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        start_new_session=True,\n",
    "        env=env,\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    wait_vllm_ready()\n",
    "    print(f\"[READY] {model_name}\")\n",
    "    return proc\n",
    "\n",
    "\n",
    "def stop_vllm_server(proc: subprocess.Popen):\n",
    "    print(\"[STOP] vLLM\")\n",
    "\n",
    "    if proc and proc.poll() is None:\n",
    "        try:\n",
    "            os.killpg(proc.pid, signal.SIGTERM)\n",
    "        except ProcessLookupError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            proc.wait(timeout=30)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            try:\n",
    "                os.killpg(proc.pid, signal.SIGKILL)\n",
    "            except ProcessLookupError:\n",
    "                pass\n",
    "            proc.wait(timeout=10)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    print(\"[CLEARED] CUDA cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe7439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_wiki_for_model(model_name: str, encoder, encoder_device):\n",
    "    metrics = await run_wiki_benchmark(\n",
    "        api=VLLM_API,\n",
    "        key=VLLM_KEY,\n",
    "        model_name=model_name,\n",
    "        concurrency=BENCH_CONCURRENCY,\n",
    "        output_dir=\"collective_results\",\n",
    "        number_of_articles=20,\n",
    "        encoder_name=\"sergeyzh/BERTA\",\n",
    "\n",
    "        device=\"cuda\",\n",
    "\n",
    "        prepare_env=False,\n",
    "        neighbor_count=0,\n",
    "        description_mode=True,\n",
    "        clusterization_with_hint=True,\n",
    "\n",
    "        shared_encoder=encoder,\n",
    "        shared_device=encoder_device,\n",
    "    )\n",
    "\n",
    "    print(f\"[DONE] {model_name}\")\n",
    "    print(metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c07c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START vLLM: YandexGPT-5-Lite-8B-instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 10:08:10 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | WikiBench initialized: model=YandexGPT-5-Lite-8B-instruct, articles=20\n",
      "2026-03-01 10:08:10 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Loading enviroment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[READY] YandexGPT-5-Lite-8B-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 10:08:10 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Enviroment loaded!\n",
      "2026-03-01 10:08:10 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Stage: rank_query started\n",
      "rank_query:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 17/20 [07:21<01:21, 27.23s/article]2026-03-01 10:15:33 | ERROR | wikibench.YandexGPT-5-Lite-8B-instruct | rank_query failed for article=Uncharted 4: A Thief’s End\n",
      "Traceback (most recent call last):\n",
      "  File \"/workdir/WikiBench/src/wiki_bench.py\", line 151, in rank_query\n",
      "    ranked_docs = await self.wiki_agent.create_ranking(article_name=article_name)\n",
      "  File \"/workdir/WikiBench/src/wiki_agent.py\", line 71, in create_ranking\n",
      "    top_k = self.utils.get_number_of_snippets()[article_name] * 3 # proportion is 1:2 (1 - relevant, 2 - irrelevant)\n",
      "KeyError: 'Uncharted 4: A Thief’s End'\n",
      "rank_query: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [07:52<00:00, 23.65s/article]\n",
      "2026-03-01 10:16:03 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Final result for stage rank_query: (0.6349161892582756, 0.08697770832263775)\n",
      "2026-03-01 10:16:03 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Stage: rank_outline started: neighbor_count=0, description_mode=True, clusterization_with_hint=True\n",
      "rank_outline: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [03:11<00:00,  9.59s/article]\n",
      "2026-03-01 10:19:15 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Final result for stage rank_outline: P=0.5952 [0.5744; 0.6200] | R=0.6199 [0.5947; 0.6477] | F=0.6063 [0.5857; 0.6318]\n",
      "2026-03-01 10:19:15 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Stage: rank_sections started\n",
      "rank_sections: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [08:51<00:00, 26.55s/article]\n",
      "2026-03-01 10:28:06 | INFO | wikibench.YandexGPT-5-Lite-8B-instruct | Final result for stage rank_sections: P=0.5302 [0.5157; 0.5448] | R=0.5728 [0.5566; 0.5903] | F=0.5403 [0.5295; 0.5513] | Rouge=0.1510 [0.1375; 0.1648] | BLEU=0.0332 [0.0269; 0.0402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] YandexGPT-5-Lite-8B-instruct\n",
      "{'model_name': 'YandexGPT-5-Lite-8B-instruct', 'number_of_articles': 20, 'ranking': {'ndcg_mean': 0.6349161892582756, 'r_precision_mean': 0.08697770832263775}, 'ranking_time': 472.97353360801935, 'outline': {'precision': {'mean': 0.5951797366142273, 'ci_low': 0.5744386240839958, 'ci_high': 0.6200312227010727}, 'recall': {'mean': 0.6198519468307495, 'ci_low': 0.5946791410446167, 'ci_high': 0.6476522892713547}, 'f1': {'mean': 0.6063086272420538, 'ci_low': 0.5857036395015154, 'ci_high': 0.6318296041174258}}, 'outline_time': 191.77827513962984, 'sections': {'precision': {'mean': 0.5302366018295288, 'ci_low': 0.5157355085015297, 'ci_high': 0.5448180615901946}, 'recall': {'mean': 0.5728142559528351, 'ci_low': 0.5565543726086617, 'ci_high': 0.590296696126461}, 'f1': {'mean': 0.5403300451022222, 'ci_low': 0.5295421265284487, 'ci_high': 0.5513359149299054}, 'rouge_l': {'mean': 0.1510034144871254, 'ci_low': 0.13747583569379748, 'ci_high': 0.16484155835566705}, 'bleu': {'mean': 0.03320050855223282, 'ci_low': 0.026875050298704024, 'ci_high': 0.04022099016707876}}, 'sections_time': 531.4697125963867}\n",
      "[STOP] vLLM\n",
      "[CLEARED] CUDA cache\n",
      "\n",
      "=== ALL METRICS ===\n",
      "YandexGPT-5-Lite-8B-instruct {'model_name': 'YandexGPT-5-Lite-8B-instruct', 'number_of_articles': 20, 'ranking': {'ndcg_mean': 0.6349161892582756, 'r_precision_mean': 0.08697770832263775}, 'ranking_time': 472.97353360801935, 'outline': {'precision': {'mean': 0.5951797366142273, 'ci_low': 0.5744386240839958, 'ci_high': 0.6200312227010727}, 'recall': {'mean': 0.6198519468307495, 'ci_low': 0.5946791410446167, 'ci_high': 0.6476522892713547}, 'f1': {'mean': 0.6063086272420538, 'ci_low': 0.5857036395015154, 'ci_high': 0.6318296041174258}}, 'outline_time': 191.77827513962984, 'sections': {'precision': {'mean': 0.5302366018295288, 'ci_low': 0.5157355085015297, 'ci_high': 0.5448180615901946}, 'recall': {'mean': 0.5728142559528351, 'ci_low': 0.5565543726086617, 'ci_high': 0.590296696126461}, 'f1': {'mean': 0.5403300451022222, 'ci_low': 0.5295421265284487, 'ci_high': 0.5513359149299054}, 'rouge_l': {'mean': 0.1510034144871254, 'ci_low': 0.13747583569379748, 'ci_high': 0.16484155835566705}, 'bleu': {'mean': 0.03320050855223282, 'ci_low': 0.026875050298704024, 'ci_high': 0.04022099016707876}}, 'sections_time': 531.4697125963867}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ГЛАВНЫЙ ЦИКЛ\n",
    "# =========================\n",
    "\n",
    "async def main():\n",
    "    logging.getLogger(\"sentence_transformers.SentenceTransformer\").setLevel(logging.ERROR)\n",
    "\n",
    "    encoder_device = torch.device(\"cuda\")\n",
    "    encoder = SentenceTransformer(\"sergeyzh/BERTA\").to(encoder_device)\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    for spec in MODEL_SPECS:\n",
    "        model_name = spec[\"alias\"]\n",
    "        model_path = BASE_MODELS_DIR / alias.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "        proc = None\n",
    "\n",
    "        try:\n",
    "            if not Path(model_path).exists():\n",
    "                raise FileNotFoundError(f\"Не найдена папка модели: {model_path}\")\n",
    "\n",
    "            proc = start_vllm_server(model_name, model_path)\n",
    "            metrics = await run_wiki_for_model(model_name, encoder, encoder_device)\n",
    "            all_metrics[model_name] = metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {model_name}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if proc is not None:\n",
    "                stop_vllm_server(proc)\n",
    "\n",
    "    print(\"\\n=== ALL METRICS ===\")\n",
    "    for name, metrics in all_metrics.items():\n",
    "        print(name, metrics)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717291fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
